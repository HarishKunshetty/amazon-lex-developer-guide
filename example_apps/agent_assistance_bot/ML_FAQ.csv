What is Amazon SageMaker?,"Amazon SageMaker is a fully managed service that provides every developer and data scientist with the ability to build, train, and deploy machine learning (ML) models quickly. SageMaker removes the heavy lifting from each step of the machine learning process to make it easier to develop high quality models."
In which regions is Amazon SageMaker available?,"For a list of the supported Amazon SageMaker AWS regions, please visit the AWS Region Table for all AWS global infrastructure. Also for more information, see Regions and Endpoints in the AWS General Reference."
What is the service availability of Amazon SageMaker?,"Amazon SageMaker is designed for high availability. There are no maintenance windows or scheduled downtimes. SageMaker APIs run in Amazon?s proven, high-availability data centers, with service stack replication configured across three facilities in each AWS region to provide fault tolerance in the event of a server failure or Availability Zone outage."
What security measures does Amazon SageMaker have?,"Amazon SageMaker ensures that ML model artifacts and other system artifacts are encrypted in transit and at rest. Requests to the SageMaker API and console are made over a secure (SSL) connection. You pass AWS Identity and Access Management roles to SageMaker to provide permissions to access resources on your behalf for training and deployment. You can use encrypted S3 buckets for model artifacts and data, as well as pass a KMS key to SageMaker notebooks, training jobs, and endpoints, to encrypt the attached ML storage volume."
How does Amazon SageMaker secure my code?,"Amazon SageMaker stores code in ML storage volumes, secured by security groups and optionally encrypted at rest."
How am I charged for Amazon SageMaker?,"You pay for ML compute, storage, and data processing resources you use for hosting the notebook, training the model, performing predictions, and logging the outputs. Amazon SageMaker allows you to select the number and type of instance used for the hosted notebook, training, and model hosting. You only pay for what you use, as you use it; there are no minimum fees and no upfront commitments. See the Amazon SageMaker pricing page for details."
"What if I have my own notebook, training, or hosting environment?","Amazon SageMaker provides a full end-to-end workflow, but you can continue to use your existing tools with SageMaker. You can easily transfer the results of each stage in and out of SageMaker as your business requirements dictate."
Is R supported with Amazon SageMaker?,"Yes, R is supported with Amazon SageMaker. You can use R within SageMaker Notebook instances, which include a pre-installed R kernel and the reticulate library. Reticulate offers an R interface for the Amazon SageMaker Python SDK, enabling machine learning practitioners to build, train, tune, and deploy R models.?"
What is Amazon SageMaker Studio?,"Amazon SageMaker Studio provides a single, web-based visual interface where you can perform all ML development steps. SageMaker Studio gives you complete access, control, and visibility into each step required to build, train, and deploy models. You can quickly upload data, create new notebooks, train and tune models, move back and forth between steps to adjust experiments, compare results, and deploy models to production all in one place, making you much more productive. All ML development activites including notebooks, experiment management, automatic model creation, debugging and profiling, and model drift detection can be performed within the unified SageMaker Studio visual interface."
What is Amazon SageMaker Autopilot?,"Amazon SageMaker Autopilot is the industry?s first automated machine learning capability that gives you complete control and visibility into your ML models. SageMaker Autopilot automatically inspects raw data, applies feature processors, picks the best set of algorithms, trains and tunes multiple models, tracks their performance, and then ranks the models based on performance, all with just a few clicks. The result is the best performing model that you can deploy at a fraction of the time normally required to train the model. You get full visibility into how the model was created and what?s in it and SageMaker Autopilot integrates with Amazon SageMaker Studio. You can explore up to 50 different models generated by SageMaker Autopilot inside SageMaker Studio so its easy to pick the best model for your use case. SageMaker Autopilot can be used by people without machine learning experience to easily produce a model or it can be used by experienced developers to quickly develop a baseline model on which teams can further iterate."
How is Amazon SageMaker Autopilot different from vertical AI services like Amazon Personalize and Amazon Forecast?,"While Amazon Personalize and Amazon Forecast specifically target at personalized recommendation and forecasting use cases, Amazon SageMaker Autopilot is a generic automatic machine learning solution for classification and regression problems, such as fraud detection, churn analysis, and targeted marketing. Personalize and Forecast focus on simplifying end to end experience by offering training and model hosting in a bundle. You can train models using Amazon SageMaker Autopilot and get full access to the models as well as the pipelines that generated the models. They can then deploy the models to the hosting environment of their choice, or further iterate to improve model quality."
What built-in algorithms are supported in Amazon SageMaker Autopilot?,Amazon SageMaker Autopilot supports 2 built-in algorithms at launch: XGBoost and Linear Learner.
Does Amazon SageMaker Autopilot support distributed training?,Yes. All Amazon SageMaker Autopilot built-in algorithms support distributed training out of the box.
Can I stop an Amazon SageMaker Autopilot job manually?,"Yes. You can stop a job at any time. When an Amazon SageMaker Autopilot job is stopped, all ongoing trials will be stopped and no new trial will be started."
What are Amazon SageMaker Studio Notebooks?,"Amazon SageMaker Studio Notebook is a new collaborative, flexible, managed Jupyter notebook experience that is part of Amazon SageMaker Studio, a fully integrated development environment for machine learning."
How are SageMaker Studio Notebooks different from the instance based notebooks offering?,"SageMaker Studio Notebooks offers a few important features that differentiate it from the instance based notebooks. With the new notebook experience, you can now quickly launch notebooks without needing to manually provision an instance and waiting for it to be operational. The start-up time of launching the UI to read and execute a notebook is faster than the instance based notebooks. You also have the flexibility to choose from a large collection of instance types from within the UI at any time. You will no longer need to go to the AWS console to start new instances and port over your notebooks. Each user has a isolated home directory independent of a particular instance. This directory is automatically mounted into all notebook servers and kernels as they?re started, so you can access your notebooks and other files even when you switch instances to view and run your notebooks. SageMaker Studio Notebooks are integrated with AWS SSO, making it easy to use your organizational credentials to access the notebooks. Notebook sharing is an integrated feature in SageMaker Studio Notebooks. You can also share your notebooks with your peers using a single click."
What types of notebooks are supported?,"Currently, Jupyter notebooks are supported."
How do Amazon SageMaker Studio Notebooks work?,"Amazon SageMaker Studio Notebooks are one-click Jupyter notebooks that can be spun quickly. The underlying compute resources are fully elastic, so you can easily dial up or down the available resources and the changes take place automatically in the background without interrupting your work. SageMaker also enables one-click sharing of notebooks. You can easily share notebooks with others and they?ll get the exact same notebook, saved in the same place. With SageMaker Studio Notebooks you can sign in with your corporate credentials using AWS SSO. Sharing notebooks within and across teams is easy, since the dependencies needed to run a notebook are automatically tracked in work images that are encapsulated with the notebook as it is shared."
How do Amazon SageMaker Studio Notebooks work with other AWS services?,"Amazon SageMaker Studio Notebooks give you access to all SageMaker features, such as distributed training, batch transform, hosting, and experiment management. You can access other services such as datasets in Amazon S3, Amazon Redshift, AWS Glue, Amazon EMR, or AWS Lake Formation from SageMaker Notebooks."
What is Amazon SageMaker Ground Truth?,"Amazon SageMaker Ground Truth provides automated data labeling using machine learning. SageMaker Ground Truth will first select a random sample of data and send it to Amazon Mechanical Turk to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a threshold you set. Where the confidence score falls below your threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans."
What is Amazon SageMaker Experiments?,"Amazon SageMaker Experiments helps you organize and track iterations to machine learning models. SageMaker Experiments helps you manage iterations by automatically capturing the input parameters, configurations, and results, and storing them as ?experiments?. You can work within the visual interface of SageMaker Studio, where you can browse active experiments, search for previous experiments by their characteristics, review previous experiments with their results, and compare experiment results visually."
What is Amazon SageMaker Debugger?,"Amazon SageMaker Debugger makes the training process more transparent by automatically capturing real-time metrics during training such as training and validation, confusion matrices, and learning gradients to help improve model accuracy. The metrics from SageMaker Debugger can be visualized in Amazon SageMaker Studio for easy understanding. SageMaker Debugger can also generate warnings and remediation advice when common training problems are detected. With SageMaker Debugger, you can interpret how a model is working, representing an early step towards model explainability."
What is Managed Spot Training?,"Managed Spot Training with Amazon SageMaker lets you train your machine learning models using Amazon EC2 Spot instances, while reducing the cost of training your models by up to 90%."
How do I use Managed Spot Training?,"You enable the Managed Spot Training option when submitting your training jobs and you also specify how long you want to wait for Spot capacity. Amazon SageMaker will then use Amazon EC2 Spot instances to run your job and manages the Spot capacity. You have full visibility into the status of your training job, both while they are running and while they are waiting for capacity."
When should I use Managed Spot Training?,"Managed Spot Training is ideal when you have flexibility with your training runs and when you want to minimize the cost of your training jobs. With Managed Spot Training, you can reduce the cost of training your machine learning models by up to 90%."
How does Managed Spot Training work?,"Managed Spot Training uses Amazon EC2 Spot instances for training, and these instances can be pre-empted when AWS needs capacity. As a result, Managed Spot Training jobs can run in small increments as and when capacity becomes available. The training jobs need not be restarted from scratch when there is an interruption as Amazon SageMaker can resume the training jobs using the latest model checkpoint. The built-in frameworks and the built-in computer vision algorithms with SageMaker enable periodic checkpoints, and you can enable checkpoints with custom models."
Do I need to periodically checkpoint with Managed Spot Training?,"We recommend periodic checkpoints as a general best practice for long running training jobs. This prevents your Managed Spot Training jobs from restarting if capacity is pre-empted. When you enable checkpoints, Amazon SageMaker resumes your Managed Spot Training jobs from the last checkpoint."
How do you calculate the cost savings with Managed Spot Training jobs?,"Once a Managed Spot Training job is completed, you can see the savings in the AWS management console and also calculate the cost savings as the percentage difference between the duration for which the training job ran and the duration for which you were billed. Regardless of how many times your Managed Spot Training jobs are interrupted, you are charged only once for the duration for which the data was downloaded."
Which instances can I use with Managed Spot Training?,Managed Spot Training can be used with all instances supported in Amazon SageMaker.
Which AWS regions are supported with Managed Spot Training?,Managed Spot Training is supported on all AWS regions where Amazon SageMaker is currently available.
Are there limits to the size of the dataset I can use for training?,There are no fixed limits to the size of the dataset you can use for training models with Amazon SageMaker.
What data sources can I easily pull into Amazon SageMaker?,You can specify the Amazon S3 location of your training data as part of creating a training job.
What algorithms does Amazon SageMaker use to generate models?,"Amazon SageMaker includes built-in algorithms for linear regression, logistic regression, k-means clustering, principal component analysis, factorization machines, neural topic modeling, latent dirichlet allocation, gradient boosted trees, sequence2sequence, time series forecasting, word2vec, and image classification. SageMaker also provides optimized Apache MXNet, Tensorflow, Chainer, PyTorch, Gluon, Keras, Horovod, Scikit-learn, and Deep Graph Library containers. In addition, Amazon SageMaker supports your custom training algorithms provided through a Docker image adhering to the documented specification."
What is Automatic Model Tuning?,Most machine learning algorithms expose a variety of parameters that control how the underlying algorithm operates. Those parameters are generally referred to as hyperparameters and their values affect the quality of the trained models. Automatic model tuning is the process of finding a set of hyperparameters for an algorithm that can yield an optimal model.
What models can be tuned with Automatic Model Tuning?,"You can run automatic model tuning in Amazon SageMaker on top of any algorithm as long as it?s scientifically feasible, including built-in SageMaker algorithms, deep neural networks, or arbitrary algorithms you bring to SageMaker in the form of Docker images."
Can I use Automatic Model Tuning outside of Amazon SageMaker?,Not at this time. The best model tuning performance and experience is within Amazon SageMaker.
What is the underlying tuning algorithm?,"Currently, our algorithm for tuning hyperparameters is a customized implementation of Bayesian Optimization. It aims to optimize a customer specified objective metric throughout the tuning process. Specifically, it checks the object metric of completed training jobs, and leverages the knowledge to infer the hyperparameter combination for the next training job."
Will you recommend specific hyperparameters for tuning?,"No. How certain hyperparameters impact the model performance depends on various factors and it is hard to definitively say one hyperparameter is more important than the others and thus needs to be tuned. For built-in algorithms within Amazon SageMaker, we do call out whether or not a hyperparameter is tunable."
How long does a hyperparameter tuning job take?,"The length of time for a hyperparameter tuning job depends on multiple factors including the size of the data, the underlying algorithm, and the values of the hyperparameters. Additionally, customers can choose the number of simultaneous training jobs and total number of training jobs. All these choices affect how long a hyperparameter tuning job can last"
Can I optimize multiple objectives simultaneously like a model to be both fast and accurate?,"Not at this time. Right now, you need to specify a single objective metric to optimize or change your algorithm code to emit a new metric, which is a weighted average between two or more useful metrics, and have the tuning process optimize towards that objective metric."
How much does Automatic Model Tuning cost?,"There is no charge for a hyperparameter tuning job itself. You will be charged by the training jobs that are launched by the hyperparameter tuning job, based on model training pricing."
How do I decide to use Amazon SageMaker Autopilot or Automatic Model Tuning?,"Amazon SageMaker Autopilot automates everything in a typical machine learning workflow, including feature preprocessing, algorithm selection, and hyperparameter tuning, while specifically focusing on classification and regression use cases. Automatic Model Tuning, on the other hand, is designed to tune any model, no matter whether it is based on built-in algorithms, deep learning frameworks, or custom containers. In exchange for the flexibility, you have to manually pick the specific algorithm, determine the hyperparameters to tune, and corresponding search ranges."
What is reinforcement learning?,Reinforcement learning is a machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback from its own actions and experiences.
Can I train reinforcement learning models in Amazon SageMaker?,"Yes, you can train reinforcement learning models in Amazon SageMaker in addition to supervised and unsupervised learning models."
How is reinforcement learning different from supervised learning?,"Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning where the feedback provided to the agent is correct set of actions for performing a task, reinforcement learning uses a delayed feedback where reward signals are optimized to ensure a long-term goal through a sequence of actions."
When should I use reinforcement learning?,"While the goal of supervised learning techniques is to find the right answer based on the patterns in the training data, the goal of unsupervised learning techniques is to find similarities and differences between data points. In contrast, the goal of reinforcement learning techniques is to learn how to achieve a desired outcome even when it is not clear how to accomplish that outcome. As a result, RL is more suited to enabling intelligent applications where an agent can make autonomous decisions such as robotics, autonomous vehicles, HVAC, industrial control, and more."
What type of environments can I use for training reinforcement learning models?,"Amazon SageMaker RL supports a number of different environments for training reinforcement learning models. You can use AWS services such as AWS RoboMaker, open source environments or custom environments developed using Open AI Gym interfaces, or commercial simulation environments such as MATLAB and SimuLink."
Do I need to write my own RL agent algorithms to train reinforcement learning models?,"No, Amazon SageMaker RL includes RL toolkits such as Coach and Ray RLLib that offer implementations of RL agent algorithms such as DQN, PPO, A3C, and many more."
Can I bring my own RL libraries and algorithm implementation and run in Amazon SageMaker RL?,"Yes, you can bring your own RL libraries and algorithm implementations in Docker Containers and run those in Amazon SageMaker RL."
Can I do distributed rollouts using Amazon SageMaker RL?,Yes. You can even select a heterogeneous cluster where the training can run on a GPU instance and the simulations can run on multiple CPU instances.
What is Amazon SageMaker Model Monitor?,"Amazon SageMaker Model Monitor allows developers to detect and remediate concept drift. SageMaker Model Monitor automatically detects concept drift in deployed models and provides detailed alerts that help identify the source of the problem. All models trained in SageMaker automatically emit key metrics that can be collected and viewed in SageMaker Studio. From inside SageMaker Studio you can configure data to be collected, how to view it, and when to receive alerts."
Can I access the infrastructure that Amazon SageMaker runs on?,"No. Amazon SageMaker operates the compute infrastructure on your behalf, allowing it to perform health checks, apply security patches, and do other routine maintenance. You can also deploy the model artifacts from training with custom inference code in your own hosting environment."
How do I scale the size and performance of an Amazon SageMaker model once in production?,"Amazon SageMaker hosting automatically scales to the performance needed for your application using Application Auto Scaling. In addition, you can manually change the instance number and type without incurring downtime through modifying the endpoint configuration."
How do I monitor my Amazon SageMaker production environment?,"Amazon SageMaker emits performance metrics to Amazon CloudWatch Metrics so you can track metrics, set alarms, and automatically react to changes in production traffic. In addition, Amazon SageMaker writes logs to Amazon Cloudwatch Logs to let you monitor and troubleshoot your production environment."
What kinds of models can be hosted with Amazon SageMaker?,Amazon SageMaker can host any model that adheres to the documented specification for inference Docker images. This includes models created from Amazon SageMaker model artifacts and inference code.
How many concurrent real-time API requests does Amazon SageMaker support?,Amazon SageMaker is designed to scale to a large number of transactions per second. The precise number varies based on the deployed model and the number and type of instances to which the model is deployed.
What is Batch Transform?,"Batch Transform enables you to run predictions on large or small batch data. There is no need to break down the data set into multiple chunks or managing real-time endpoints. With a simple API, you can request predictions for a large number of data records and transform the data quickly and easily"
What is Amazon SageMaker Neo?,Amazon SageMaker Neo enables machine learning models to train once and run anywhere in the cloud and at the edge. SageMaker Neo automatically optimizes models built with popular deep learning frameworks that can be used to deploy on multiple hardware platforms. Optimized models run up to two times faster and consume less than a tenth of the resources of typical machine learning models.
How do I get started with Amazon SageMaker Neo?,"To get started with Amazon SageMaker Neo, you log into the Amazon SageMaker console, choose a trained model, follow the example to compile models, and deploy the resulting model onto your target hardware platform."
What are the major components of Amazon SageMaker Neo?,"Amazon SageMaker Neo contains two major components ? a compiler and a runtime. First, the Neo compiler reads models exported by different frameworks. It then converts the framework-specific functions and operations into a framework-agnostic intermediate representation. Next, it performs a series of optimizations. Then, the compiler generates binary code for the optimized operations and writes them to a shared object library. The compiler also saves the model definition and parameters into separate files. During execution, the Neo runtime loads the artifacts generated by the compiler -- model definition, parameters, and the shared object library to run the model."
Do I need to use Amazon SageMaker to train my model in order to use Amazon SageMaker Neo to convert the model?,No. You can train models elsewhere and use Neo to optimize them for Amazon SageMaker ML instances or AWS IoT Greengrass supported devices.
Which models does Amazon SageMaker Neo support?,"Currently, Amazon SageMaker Neo supports the most popular deep learning models that power computer vision applications and the most popular decision tree models used in Amazon SageMaker today. Neo optimizes the performance of AlexNet, ResNet, VGG, Inception, MobileNet, SqueezeNet, and DenseNet models trained in MXNet and TensorFlow, and classification and random cut forest models trained in XGBoost."
Which platforms does Amazon SageMaker Neo support?,"Currently, Neo supports SageMaker ML.C5, ML.C4, ML.M5, ML.M4, ML.P3, and ML.P2 instances and AWS DeepLens, Raspberry Pi, and Jetson TX1 and TX2 devices, and Greengrass devices-based Intel? Atom and Intel? Xeon CPUs, ARM Cortex-A CPUs, and Nvidia Maxwell and Pascal GPUs."
Do I need to use a specific version of a framework that is supported on the target hardware?,No. Developers can run models using the Amazon SageMaker Neo container without dependencies on the framework.
How much does it cost to use Amazon SageMaker Neo?,You pay for the use of the Amazon SageMaker ML instance that runs inference using Amazon SageMaker Neo.
In which AWS regions is Amazon SageMaker Neo available?,"To see a list of support regions, view the AWS region table."
What is AWS DeepLens?,"AWS DeepLens is the world?s first deep-learning enabled video camera for developers of all skill levels to grow their machine learning skills through hands-on computer vision tutorials, example code, and pre-built models."
What is AWS DeepLens (2019 Edition)?,"The AWS DeepLens (2019 Edition) is available for customers in the USA, Canada, UK, Germany, France, Spain, Italy, and Japan. We've made improvements throughout the experience: devices are now easier to setup, allowing developers to get started with machine learning even more quickly; many ML models will run 2x faster on the device thanks to optimization with SageMaker Neo. In addition to the device improvements, new educational content is available to all console users to help making learning ML fun with AWS DeepLens. This includes guided instructions for building ML applications for interesting use cases like monitoring worker safety, performing sentiment analysis, and tracking how much coffee is being drunk in the office."
How is AWS DeepLens different from other video cameras in the market?,"AWS DeepLens is the world's first video camera optimized to run machine learning models and perform inference on the device. It comes with 6 sample projects at launch, that you can deploy to your AWS DeepLens in less than 10 minutes. You can run the sample projects as is, connect them with other AWS services, train a model in Amazon Sagemaker and deploy it to AWS DeepLens, or extend the functionality by triggering a lambda function when an action takes place. You can also apply more advanced analysis on the cloud using Amazon Rekognition. AWS DeepLens provides the building blocks for your machine learning needs."
What sample projects are available?,"There are 7 sample projects available. We will continue to launch practical and fun projects for developers to use and learn, based on user feedback. The 7 sample projects are: There are 7 sample projects available. We will continue to launch practical and fun projects for developers to use and learn, based on user feedback. The 7 sample projects are:

1. Object Detection

2. Hot Dog Not Hot Dog

3. Cat and Dog

4. Artistic Style Transfer

5. Activity Detection

6. Face Detection

7. Bird Classification"
What geographic regions is AWS DeepLens available?,"AWS DeepLens (2019 Edition) is available in the US, Germany, France, Italy, Spain, UK, Japan and Canada."
Does AWS DeepLens include Alexa?,"No, AWS DeepLens does not have Alexa or any far-field audio capabilities. However, AWS DeepLens has a 2D microphone array that is capable of running custom audio models, with additional programming required."
How can I get a AWS DeepLens?,"AWS DeepLens (2019 Edition) is now available for pre-order for developers in Canada, Europe, and Japan via the Amazon.ca, Amazon.de, Amazon.es, Amazon.fr, Amazon.it, Amazon.co.jp, and Amazon.co.uk websites."
In what regions will the AWS DeepLens console be available?,"AWS DeepLens will be available in us- east-1 (N.Virginia), eu-central-1 (Frankfurt) and ap-northeast-1 (Tokyo)."
What are the product specifications of the device?,"
    Intel Atom? Processor
    Gen9 graphics
    Ubuntu OS 16.04 LTS
    100 GFLOPS performance
    Dual band Wi-Fi
    8GB RAM
    16GB storage
    Expandable storage via microSD card
    4MP camera with MJPEG
    H.264 encoding at 1080p resolution
    2 USB ports
    Micro HDMI
    Audio out"
"Why do I have ""v1.1"" marked on the bottom of my device?","AWS DeepLens (2019 Edition) is marked with ?v1.1? on the bottom of the device. We have made significant improvements to the user experience, including onboarding, tutorials and additional sensor compatibility support such as depth sensor from Intel Real Sense.?The orginal AWS DeepLens cannot be upgraded to v1.1 via software updates. Some of the device modifications including the simplified onboarding were hardware changes."
What deep learning frameworks can I run on the device?,"AWS DeepLens (2019 Edition) is optimized for Apache MXNet, TensorFlow and Caffe.?"
What kind of performance can I expect with AWS DeepLens?,"Performance is measured on images inferred per second and latency. Different models will have varying inference per second. The baseline inference performance is 14 images/second on AlexNet, and 5 images/second on ResNet 50 for batch size of 1. The characteristics of the network that the DeepLens is connected to will determine the latency performance."
What MXNet network architecture layers does AWS DeepLens support?,"AWS DeepLens offers support for 20 different network architecture layers. The layers supported are:AWS DeepLens offers support for 20 different network architecture layers. The layers supported are:

    Activation
    BatchNorm
    Concat
    Convolution
    elemwise_add
    Pooling
    Flatten
    FullyConnected
    InputLayer
    UpSampling
    Reshape
    ScaleShift
    SoftmaxActivation
    SoftmaxOutput
    transpose
    _contrib_MultiBoxPrior
    _contrib_MultiBoxDetection
    _Plus
    Deconvolution
    _mul
"
What comes in the box and how do I get started?,"Inside the box, developers will find a Getting Started guide, the AWS DeepLens device, a region specific power cord and adapter, USB cable and a 32GB microSD card. Setup and configuration of the DeepLens device can be done in minutes using the AWS DeepLens console, and by configuring the device through a browser on your laptop or PC. There are three 10-Minute Tutorials designed to help guide you through getting started:

1. Create and Deploy a Project
2. Extend a Project
3. Build an AWS DeepLens Project with Amazon SageMaker"
Why is an USB port marked as registration?,"On AWS DeepLens (2019 Edition) the USB port marked as registration will be used during the onboarding process to register your AWS DeepLens to your AWS account. The USB port for registration is configured as a slave port. Hence, it cannot be used for keyboard or other master port setup. If you need more ports to connect, we recommend using a USB hub. "
Can I train my models on the device?,"No, AWS DeepLens is capable of running inference or predictions using trained models. You can train your models in Amazon SageMaker, a machine learning platform to train and host your models. AWS DeepLens offers a simple 1-click deploy feature to publish trained models from Amazon SageMaker."
What AWS services are integrated with AWS DeepLens?,"DeepLens is pre-configured for integration with AWS Greengrass, Amazon SageMaker and Amazon Kinesis Video Streams. You can integrate with many other AWS services, such as Amazon S3, Amazon Lambda, Amazon Dynamo, Amazon Rekognition using AWS DeepLens."
Can I SSH into AWS DeepLens?,"Yes, we have designed AWS DeepLens to be easy to use, yet accessible for advanced developers. You can SSH into the device using the command: ssh aws_cam@"
What programming languages are supported by AWS DeepLens?,You can define and run models on the camera data stream locally in Python 2.7.
Do I need to be connected to internet to run the models?,"No. You can run the models that you have deployed to AWS DeepLens without being connected to the internet. However, you need internet to deploy the model from the cloud to the device initially. After transferring your model, AWS DeepLens can perform inference on the device locally without requiring cloud connectivity. However, if you have components in your project that require interaction with cloud, you will need to have internet for those components."
Can I run my own custom models on AWS DeepLens?,"Yes. You can also create your own project from scratch, using the AWS SageMaker platform to prepare data and train a model using a hosted notebook, and then publish the trained model to your AWS DeepLens for testing and refinement. You can also import an externally-trained model into AWS DeepLens by specifying the S3 location for model architecture and network weights files."
"Why do I have ""v1.1"" marked on the bottom of my device?","AWS DeepLens (2019 Edition) is marked with ?v1.1? on the bottom of the device. We have made significant improvements to the user experience, including onboarding, tutorials and additional sensor compatibility support such as depth sensor from Intel Real Sense.?"
What is AWS DeepLens (2019 Edition)?,"The AWS DeepLens (2019 Edition) includes an optimized onboarding process that allows developers to get started with machine learning quickly, support for Intel? RealSense? depth sensor that allows you to build advanced machine learning models with higher accuracy by utilizing not just vision as the input parameter but also depth, and support for the Intel? Movidius? Neural Compute Stick for those who want faster computing speeds. The AWS DeepLens (2019 Edition) also comes with Amazon SageMaker Neo integration which lets customers train models once and run them with up to 2X improvement in performance. In addition to the device improvements, we have invested in new content to help making learning ML fun with AWS DeepLens. This includes guided instructions for building ML applications for interesting use cases such as worker safety, sentiment analysis, who drinks the most coffee, to name a few. All these enhancements are now available for customers in the USA, Canada, UK, Germany, France, Spain, Italy, Japan. "
What is Amazon Polly?,"Amazon Polly is a service that turns text into lifelike speech. Amazon Polly enables existing applications to speak as a first class feature and creates the opportunity for entirely new categories of speech-enabled products, from mobile apps and cars, to devices and appliances. Amazon Polly includes dozens of lifelike voices and support for multiple languages, so you can select the ideal voice and distribute your speech-enabled applications in many geographies. Amazon Polly is easy to use ? you just send the text you want converted into speech to the Amazon Polly API, and Amazon Polly immediately returns the audio stream to your application so you can play it directly or store it in a standard audio file format, such as MP3. Amazon Polly supports Speech Synthesis Markup Language (SSML) tags like prosody so you can adjust the speech rate, pitch, or volume. Amazon Polly is a secure service that delivers all of these benefits at high scale and at low latency. You can cache and replay Amazon Polly?s generated speech at no additional cost. Amazon Polly lets you convert millions of characters per month for free during the first year, upon sign-up. Amazon Polly?s pay-as-you-go pricing, low cost per request, and lack of restrictions on storage and reuse of voice output make it a cost-effective way to enable speech synthesis everywhere."
Why should I use Amazon Polly?,"You can use Amazon Polly to power your application with high-quality spoken output. This cost-effective service has very low response times, and is available for virtually any use case, with no restrictions on storing and reusing generated speech"
What features are available?,"You can control various aspects of speech such as pronunciation, volume, pitch, speech rate, etc. using standardized Speech Synthesis Markup Language (SSML). You can synthesize speech for certain Neural voices using the Newscaster style, to make them sound like a TV or Radio newscaster. You can detect when specific words or sentences in the text are being spoken to the user based on the metadata included in the audio stream. This allows the developer to synchronize graphical highlighting and animations, such as the lip movements of an avatar, with the synthesized speech. You can modify the pronunciation of particular words, such as company names, acronyms, foreign words and neologisms, e.g. ?P!nk?, ?ROTFL?, ?C?est la vie? (when spoken in a non-French voice) using custom lexicons."
What are Speech Marks?,"Speech Marks are designed to complement the synthesized speech that is generated from the input text. Using this metadata alongside the synthesized speech audio stream, customers can provide their application with an enhanced visual experience such as speech-synchronized animation or karaoke-style highlighting. Amazon Polly generates Speech Marks using the following four elements:

    Sentence, which indicates a sentence element in the input text to be spoken;
    Word, which Indicates a word element in the text;
    Viseme, which describes the shape of the lips that corresponds to the sound that is spoken;
    SSML, which describes an SSML element used in the text.

Speech Marks are delivered in form of a JSON stream -- specifically, a set of standalone JSON objects delimited with new lines -- containing anywhere from one to all four of these elements, when using the synthesize-speech method with the speech-mark-types parameter. You can find more information in the Amazon Polly Developer Guide."
What are the most common use cases for this service?,"With Amazon Polly, you can bring your applications to life, by adding life-like speech capabilities. For example, in E-learning and education, you can build applications leveraging Amazon Polly?s Text-to-Speech (TTS) capability to help people with reading disabilities. Amazon Polly can be used to help the blind and visually impaired consume digital content (eBooks, news etc). Amazon Polly can be used in announcement systems in public transportation and industrial control systems for notifications and emergency announcements. There are a wide range of devices such as set-top boxes, smart watches, tablets, smartphones and IoT devices, which can leverage Amazon Polly for providing audio output. Amazon Polly can be used in telephony solutions to voice Interactive Voice Response systems. Applications such as quiz games, animations, avatars or narration generation are common use-cases for cloud-based TTS solution like Amazon Polly."
How does this product work with other AWS products?,"When combined with Amazon Lex, developers can create full-blown Voice User Interfaces for their applications. Within Amazon Connect, Amazon Polly speech is used to create self-service, cloud-based contact center services. On top of that, developers of mobile applications and Internet-of-Things (IoT) solutions can leverage Amazon Polly to add spoken output to their own systems."
What are the advantages of a cloud-based Text-to-Speech solution over an on-device one?,"On-device text-to-speech solutions require significant computing resources, notably CPU power, RAM, and disk space to be available on the device. This can result in higher development cost and higher power consumption on devices such as tablets, smartphones, etc. In contrast, text-to-speech conversion done in the cloud dramatically reduces local resource requirements. This makes it possible to support all of the available languages and voices at the highest possible quality. Moreover, speech corrections and enhancements are instantly available to all end-users and do not require additional updates for all devices. Cloud-based text-to-speech (TTS) is platform independent, so it minimizes development time and effort."
How do I get started with Amazon Polly?,Simply login to your AWS account and navigate to the Amazon Polly console (which is a part of the AWS Console). You can then use the console to type in any text and listen to generated speech or save it as an audio file.
In which regions is the service available?,"Please refer to the AWS Global Infrastructure Region Table?for all regions supporting Amazon Polly?s Standard voices.?Neural voices are supported in the following subset of these regions: Northern Virginia, Oregon, Sydney and Dublin"
Which programming languages are supported?,"Amazon Polly supports all the programming languages included in the AWS SDK (Java, Node.js, .NET, PHP, Python, Ruby, Go, and C++) and AWS Mobile SDK (iOS/Android). Amazon Polly also supports an HTTP API so you can implement your own access layer."
Which audio formats are supported?,"With Amazon Polly, you can stream audio to your users in near real time. You can also choose from various sampling rates to optimize bandwidth and audio quality for your application. Amazon Polly supports MP3, Vorbis, and raw PCM audio stream formats."
What languages are supported?,Please refer to??documentation for the complete list of languages supported by Amazon Polly.
Does Amazon Polly have AWS service limits?,"To help guarantee the availability of AWS resources and to minimize billing risk for new customers, AWS maintains service limits for each account. When using Amazon Polly to power your application with high-quality spoken output, there are default service limits including limitations on throttling, operations, and Speech Synthesis Markup Language (SSML) use. For details, see Limits in Amazon Polly in the Amazon Polly Developer Guide. Combining Amazon Polly with other AWS services, such as AWS Batch for efficient batch processing, can help you make the most of Amazon Polly within those service limits."
Is Amazon Polly HIPAA certified?,"Amazon Polly is a HIPAA Eligible Service covered under the AWS Business Associate Addendum (AWS BAA). If you have an AWS BAA in place, Amazon Polly will use, disclose, and maintain your Protected Health Information (PHI) only as permitted by the terms of your AWS BAA, and not as specified in the Data Privacy section of the Amazon Polly FAQ."
How do I get started with Amazon Polly Brand Voice?,"If you are interested in building a Brand Voice using Amazon Polly, please reach out to your AWS Account Manager or contact us for more information."
What is the cost and timeline to build a Brand Voice?,"Every voice is unique, so it?s important that we learn more about your goals to accurately scope a Brand Voice engagement. If you are interested in building a Brand Voice using Amazon Polly, please reach out to your AWS Account Manager or contact us for more information."
How much does Amazon Polly cost?,Please see the Amazon Polly Pricing Page for current pricing information.
Can I use the service for generating static voice prompts that will be replayed multiple times?,"Yes, you can. The service does not restrict this and there are no additional costs for doing so."
Can I use the service to generate content that will be used in mass notification systems (for example on train station)?,"Yes, you can. The service does not restrict this and there are no additional costs for doing so."
"If I request 1,000 characters to be synthesized and request Speech Marks with the same 1,000 characters, will I be charged for 2,000 characters?",Yes. You will be charged for every request for speech or Speech Marks based on the number of characters you send to the service.
Does Amazon Polly participate in the AWS Free Tier?,"Yes, as part of the AWS Free Usage Tier, you can get started with Amazon Polly for free. Upon sign-up, new Amazon Polly customers can synthesize millions of characters for free each month for the first 12 months. Please see the Amazon Polly Pricing Page for current pricing information."
Do your prices include taxes?,"For details on taxes, please see Amazon Web Services Tax Help."
"Are text inputs processed by Amazon Polly stored, and how are they used by AWS?","Amazon Polly may store and use text inputs processed by the service solely to provide and maintain the service and to improve and develop the quality of Amazon Polly and other Amazon machine-learning/artificial-intelligence technologies. Use of your content is important for continuous improvement of your Amazon Polly customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information. You may opt out of having your content used to improve and develop the quality of Amazon Polly and other Amazon machine-learning/artificial-intelligence technologies by contacting AWS Support."
Who has access to my content that is processed and stored by Amazon Polly?,"Only authorized employees will have access to your content that is processed by Amazon Polly. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Do I still own my content that is processed and stored by Amazon Polly?,You always retain ownership of your content and we will only use your content with your consent.
Is the content processed by Amazon Polly moved outside the AWS region where I am using Amazon Polly?,"Any content processed by Amazon Polly is encrypted and stored at rest in the AWS region where you are using Amazon Polly. Some portion of content processed by Amazon Polly may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Polly customer experience and other Amazon machine-learning/artificial-intelligence technologies. If you opt out of having your content used to develop the quality of Amazon Polly and other Amazon machine-learning/artificial-intelligence technologies by contacting AWS Support, your content will not be stored in another AWS region. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
"Can I use Amazon Polly in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Polly Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Polly in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
What is Amazon Translate?,"Amazon Translate is a Neural Machine Translation (MT) service for translating text between supported languages. Powered by deep learning methods, the service provides high quality and affordable machine translation, enabling developers to translate company and user-authored content, or build applications requiring support across multiple languages. The service can be used via an API, enabling either real-time or batch translation of text from the source language to the target language."
What languages are covered??,"Amazon Translate supports translation between the following 55 languages and variants: Afrikaans, Albanian, Amharic, Arabic, Azerbaijani, Bengali, Bosnian, Bulgarian, Chinese Simplified, Chinese Traditional, Croatian, Czech, Danish, Dari, Dutch, English, Estonian, Farsi (Persian), Finnish, French, Canadian French, Georgian, German, Greek, Hausa, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Latvian, Malay, Norwegian, Pashto, Polish, Portuguese, Romanian, Russian, Serbian, Slovak, Slovenian, Somali, Spanish, Mexican Spanish, Swahili, Swedish, Tagalog, Tamil, Thai, Turkish, Ukrainian, Urdu, and Vietnamese. See?this documentation page?for more details."
Why should I use Amazon Translate?,"You should use Amazon Translate because it enables you to reach more customers, communicate with them more effectively, and decrease your TCO. Many businesses have large volumes of content, user or company authored; the only way to make all of it accessible in multiple languages in a timely manner is to use Machine Translation. Because Amazon Translate costs a fraction of the cost of human translation (0.05% at $15/1M characters for Amazon Translate vs $30K for human translation on average), businesses can now afford to translate content they could not before. For Language Service Providers (LSP) and value-added resellers, Amazon Translate supports business growth and expansion. With Amazon Translate, LSPs can increase productivity by as much as 50% and produce larger volumes of translation, freeing professional translators to focus on high-end creative content. Resellers can broaden their service portfolio without building new infrastructure or hiring staff."
What are the most common use cases for Amazon Translate?,"Amazon Translate is a great solution in cases where the volume of content is high, speed is critical, and a certain level of translation imperfection (usually minor) is acceptable. For example, if you need to extract insights from large volumes of text in many languages, enable customers to search your application in their language of choice, make user-authored content such as forums and support content accessible in languages other than the source, get the gist out of responses to questionnaires and surveys, or publish a first draft ? you can use Amazon Translate?s raw output. With light human post-editing, Amazon Translate can be applied to enabling customer service agents to support anyone, and translating company authored information such as specifications, comparisons of alternatives, FAQs, and support content. With more extensive post-editing, you can also use Amazon Translate to translate high-value, branded content, such as advertising and marketing materials, contracts, etc.  "
How can I use the service?,"The easiest way to get started with Amazon Translate is to use the console to translate some text. You can also call the service directly from the AWS Command Line Interface, or use one of the SDKs in the programming language of your choice to integrate with your applications. Either way, you can start using Amazon Translate for multilingual text capabilities to translate text with just a few lines of code. You can pass source text to the API and indicate the source and target languages. Amazon Translate return the text translated into the target language. There are three main ways to use the API ? first, you can integrate the API into your application to localize highly dynamic application components such as multi-participant chat, for example. Second, you can string it with other services to enable language-independent processing. For example, Database services such as Amazon Relational Database Service (RDS) can be called through AWS Lambda blueprints to enable website localization of moderately-dynamic content such as user generated reviews and forum posts. Finally, you can translate batches of documents. For example, financial services companies can translate and monitor news articles in any language; legal teams can discover materials in multiple languages related to a lawsuit (known as eDiscovery); patent attorneys can search patent repositories anywhere in the world in IP cases."
Does the service provide automatic source language detection?,"A: Amazon Translate takes plain text input and language flags to indicate the language of the source text and desired target. If the source language is unknown, Amazon Translate will identify the source language using Amazon Comprehend behind the scenes, and report that language back along with the translation to the target language."
What kind of inputs does the service support?,Amazon Translate supports plain text input in UTF-8 format.
What are the limits on the API??,"Amazon Translate real-time service calls are limited to 5,000 bytes per API call. We provide instructions on how to break up large documents into sections and paragraphs so that customers can translate text of any length. See instructions here. Amazon Translate asynchronous Batch Translation service accepts a batch of up to 5 GB in size per API call with each document not exceeding 1 MB in size and the number of documents in the S3 bucket folder not exceeding 1 million per batch.

The Amazon Translate service is highly scalable. The default limits can be found here."
Am I required to attribute the translation to Amazon? To Machine Translation??,"You are not required to attribute translations, but we do suggest that you attribute the translation to Machine Translation to inform your own customers."
Where can I get technical support? How do I submit feedback??,"For technical support, please contact AWS Customer Service. You can submit feedback through Customer Service, or by going to the Amazon Translate console and selecting the feedback option."
What does it cost?,Refer to the Amazon Translate pricing page to learn more.
What AWS regions are available for Amazon Translate?,"Please refer to the AWS Global Infrastructure Region Table. Amazon Translate Batch Translation is available in US East 1 (Northern Virginia), US East 2 (Ohio), US West 2 (Oregon), EU West 1 (Ireland), EU West 2 (London), EU Central 1 (Frankfurt), and Asia Pacific North East 2 (Seoul)."
?Are requests in which no translation occurs charged for?,"Requests where the source language equals the target language (whether user designated or automatically identified), and when an error occurs and no translation is returned, are not charged for. Requests where the content is non-translatable (e.g., ?&*^%((**&(^?) are charged for."
"Are text inputs processed by Amazon Translate stored, and how are they used by AWS?","Amazon Translate may store and use text inputs processed by the service solely to provide and maintain the service and to improve and develop the quality of Amazon Translate and other Amazon machine-learning/artificial-intelligence technologies. Use of your content is important for continuous improvement of your Amazon Translate customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority, and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Who has access to my content that is processed and stored by Amazon Translate?,"Only authorized employees will have access to your content that is processed by Amazon Translate. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Do I still own my content that is processed and stored by Amazon Translate,"You always retain ownership of your content, and we will only use your content with your consent."
Is the content processed by Amazon Translate moved outside the AWS region where I am using Amazon Translate?,"Any content processed by Amazon Translate is encrypted and stored at rest in the AWS region where you are using Amazon Translate. Some portion of content processed by Amazon Translate may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Translate customer experience and other Amazon machine-learning/artificial-intelligence technologies. Your trust, privacy, and the security of your content are our highest priority, and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
"Can I use Amazon Translate in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the AWS Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Translate in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
What is Natural Language Processing?,"Natural Language Processing (NLP) is a way for computers to analyze, understand, and derive meaning from textual information in a smart and useful way. By utilizing NLP, you can extract important phrases, sentiment, syntax, key entities such as brand, date, location, person, etc., and the language of the text."
What is Amazon Comprehend?,Amazon Comprehend is a natural language processing (NLP) service that uses machine learning to find meaning and insights in text.
What can I do with Amazon Comprehend?,"You can use Amazon Comprehend to identify the language of the text, extract key phrases, places, people, brands, or events, understand sentiment about products or services, and identify the main topics from a library of documents. The source of this text could be web pages, social media feeds, emails, or articles. You can also feed Amazon Comprehend a set of text documents, and it will identify topics (or group of words) that best represent the information in the collection. The output from Amazon Comprehend can be used to understand customer feedback, provide a better search experience through search filters and uses topics to categorize documents. ?"
How do I get started with Amazon Comprehend?,You can get started with Amazon Comprehend from the AWS console. Your free tier for 12 months starts from the time you submit your first request. See product documentation on how to use Amazon Comprehend APIs in your application.
What are the most common use cases of Amazon Comprehend?,"The most common use cases include:

Voice of customer analytics: You can gauge whether customer sentiment is positive, neutral, negative, or mixed based on the feedback you receive via support calls, emails, social media, and other online channels.

Semantic search: You can use Amazon Comprehend to provide a better search experience by enabling your search engine to index key phrases, entities, and sentiment. This enables you to focus the search on the intent and the context of the articles instead of basic keywords.  

Knowledge management and discovery: You can analyze a collection of documents and automatically organize them by topic. You can then use the topics to personalize content for your customers."
Do I have to be a natural language processing expert to use Amazon Comprehend?,"No, you don?t need NLP expertise to use Amazon Comprehend. You only need to call Amazon Comprehend?s API, and the service will handle the machine learning required to extract the relevant data from the text."
Is Amazon Comprehend a managed service?,"Amazon Comprehend is a fully managed and continuously trained service, so you don?t have to manage the scaling of resources, maintenance of code, or maintaining the training data."
Does Amazon Comprehend learn over time?,"Yes, Amazon Comprehend uses machine learning is continuously being trained to make it better for your use cases."
In which AWS regions in Amazon Comprehend available?,"For a list of the supported Amazon Comprehend AWS regions, please visit the AWS Region Table for all AWS global infrastructure. Also for more information, see Regions and Endpoints in the AWS General Reference."
What security measures does Amazon Comprehend have?,Requests to the Amazon Comprehend API and console are made over a secure (SSL) connection. You can use AWS Identity and Access Management (AWS IAM) to control which IAM users have access to specific Amazon Comprehend actions and resources.
Where do I store my data?,"You can use user Amazon Comprehend to read your data from Amazon S3. You can also write the results from Amazon Comprehend to a storage service, database, or data warehouse."
How do I know if the service can process my data?,"For text analysis APIs, you will receive an HTTP status code of 200 indicating successful processing. If your data can't be processed or exceeds service limits, you will get an appropriate HTTP error code."
How do I know if Amazon Comprehend is giving accurate results?,"The service will return a confidence score for each result. Low confidence scores mean that the service?s confidence is low that it is correct. Conversely, if the service is highly confident, the score will be closer to 1."
Can I import or use my own NLP model with Amazon Comprehend?,"No. At the current time, Comprehend does not support custom models."
How is Amazon Comprehend priced?,Refer to the Amazon Comprehend pricing page to learn more about pricing tiers and discounts.
"Are text inputs processed by Amazon Comprehend stored, and how are they used by AWS?","Amazon Comprehend may store and use text inputs processed by the service solely to provide and maintain the service and to develop and improve the quality of Amazon Comprehend and other Amazon machine-learning/artificial-intelligence?technologies. This does not apply to Amazon Comprehend Medical. Use of your content is important for continuous improvement of your Amazon Comprehend customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see?https://aws.amazon.com/compliance/data-privacy-faq/?for more information."
Who has access to my content that is processed and stored by Amazon Comprehend?,"Only authorized employees will have access to your content that is processed by Amazon Comprehend. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see the AWS data privacy FAQs for more information."
Do I still own my content that is processed and stored by Amazon Comprehend?,You always retain ownership of your content and we will only use your content with your consent.
Is the content processed by Amazon Comprehend moved outside the AWS region where I am using Amazon Comprehend?,"Any content processed by Amazon Comprehend is encrypted and stored at rest in the AWS region where you are using Amazon Comprehend. Some portion of content processed by Amazon Comprehend may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Comprehend customer experience and other Amazon machine-learning/artificial-intelligence?technologies. This does not apply Amazon Comprehend Medical. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see the?https://aws.amazon.com/compliance/data-privacy-faq/?for more information."
"Can I use Amazon Comprehend in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Comprehend Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Comprehend in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
What is Amazon Comprehend Medical?,"Amazon Comprehend Medical is a natural language processing service that makes it easy to use machine learning to extract relevant medical information from unstructured text. Using Amazon Comprehend Medical, you can quickly and accurately gather information, such as medical condition, medication, dosage, strength, and frequency from a variety of sources, like doctors? notes, clinical trial reports, and patient health records."
What can I do with Amazon Comprehend Medical?,"Amazon Comprehend Medical uses advanced machine learning models to accurately and quickly identify medical information such as medical conditions, and medication, and determine their relationship to each other, for instance, medication and dosage. You access Comprehend Medical through a simple API call, no machine learning expertise is required, no complicated rules to write, and no models to train. You can use the extracted medical information and their relationships to build applications for use cases, like clinical decision support, revenue cycle management (medical coding), and clinical trial management. Because Comprehend Medical is HIPAA eligible and can quickly identify protected health information (PHI), such as name, age, and medical record number, you can also use it to create applications that securely process, maintain, and transmit PHI.

Please note. When using Amazon Comprehend Medical to identify protected health information, please recall that the service provides confidence scores that indicate the level of confidence in the accuracy of the extracted entities. You should evaluate these confidence scores and identify the right confidence threshold for your use case. For specific compliance use cases, we recommend you use additional human review or other methods to confirm the accuracy of extracted PHI."
How do I get started with Amazon Comprehend Medical? ,You can get started with Amazon Comprehend Medical from the AWS Management console or using the SDK. Refer to this technical documentaton page for details. Amazon Comprehend Medical provides a free tier so you can test out the service. Refer to this pricing page.?
Do I have to be a natural language processing (NLP) expert to use Amazon Comprehend Medical?,"No, you don?t need NLP expertise to use Amazon Comprehend Medical. You only need to call Amazon Comprehend?s API, and the service will handle the machine learning required to extract the relevant data from the text."
Does Amazon Comprehend Medical learn over time?,"Yes, Amazon Comprehend uses machine learning and is continuously being trained to make it better for customer use cases. Amazon Comprehend Medical does not use customer data used with the service to train the models."
?In which AWS regions in Amazon Comprehend Medical available?,"For a list of the supported Amazon Comprehend AWS regions, please visit the AWS Region Table for all AWS global infrastructure."
What else should I know before using the Amazon Comprehend Medical service?,"Amazon Comprehend Medical is not a substitute for professional medical advice, diagnosis, or treatment. You and your end users are responsible for exercising your and their own discretion, experience, and judgment in determining the correctness, completeness, timeliness, and suitability of any information provided by Amazon Comprehend Medical. You and your end users are solely responsible for any decisions, advice, actions, and/or inactions based on the use of Amazon Comprehend Medical.

Amazon Comprehend Medical may not accurately identify protected health information in all circumstances, and does not meet the requirements for de-identification of protected health information in accordance with HIPAA. You are responsible for reviewing any output provided by Amazon Comprehend Medical to ensure it meets your needs."
How is Amazon Comprehend Medical priced?,Refer to the Amazon Comprehend Medical pricing page to learn more about pricing tiers.
Is the content processed by Amazon Comprehend Medical used for any purpose other than to provide and maintain the service?,"Amazon Comprehend Medical does not use content processed by the service for any reason other than to provide and maintain the service. Content processed by the service is not used to develop or improve the quality of Amazon Comprehend Medical or other Amazon machine-learning/artificial-intelligence technologies, and will not be stored in any AWS region other than the region in which you are using the service. You always retain ownership of your content and we will only use your content with your consent."
"Can I use Amazon Comprehend Medical in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Comprehend Medical Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Comprehend Medical in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13"
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
What is Amazon Lex?,"Amazon Lex is a service for building conversational interfaces using voice and text. Powered by the same conversational engine as Alexa, Amazon Lex provides high quality speech recognition and language understanding capabilities, enabling addition of sophisticated, natural language ?chatbots? to new and existing applications. Amazon Lex reduces multi-platform development effort, allowing you to easily publish your speech or text chatbots to mobile devices and multiple chat services, like Facebook Messenger, Slack, Kik, or Twilio SMS. Native interoperability with AWS Lambda, AWS MobileHub and Amazon CloudWatch and easy integration with many other services on the AWS platform including Amazon Cognito, and Amazon DynamoDB makes bot development effortless."
How can I get started with Amazon Lex? ,"To start using Amazon Lex, simply sign into the AWS Management Console and navigate to ?Lex? under the ?Artificial Intelligence? category. You must have an Amazon Web Services account to start using Amazon Lex. If you do not already have one, you will be prompted to create one during the sign-up process. Please refer to the Amazon Lex Getting Started Guide for more information."
What are the most common use cases for Amazon Lex?," The most common use-cases include:

    Informational bot ? build an automated customer support agent or bot that answers questions
    Application/Transactional bot ? build a stand-alone pizza ordering agent or a travel bot
    Enterprise Productivity bot ? build custom bots to connect to enterprise data resources
    Device Control bot? use Amazon Lex to issue control commands to connected devices"
How does Amazon Lex work with other AWS services?,"Amazon Lex leverages AWS Lambda for Intent fulfillment, Amazon Cognito for user authentication, and Amazon Polly for text to speech. In addition, AWS Mobile Hub can be used to automatically provision bots from a template."
Do I have to be a machine learning expert to use Amazon Lex?,No machine learning expertise is necessary to use Amazon Lex. Developers can declaratively specify the conversation flow and Amazon Lex will take care of the speech recognition and natural language understanding functionality. Developers provide some sample utterances in plain English and the different parameters (slots) that they would like to collect from their user with the corresponding prompts. The language model gets built automatically.
In which AWS regions is Amazon Lex available?,"For a list of the supported Amazon Lex AWS regions, please visit the AWS Region Table for all AWS global infrastructure. Also for more information, see Regions and Endpoints in the AWS General Reference."
What is the maximum bandwidth supported on Amazon Lex? ,Amazon Lex scales to your needs and does not impose bandwidth constraints.
Is Amazon Lex a managed service?,Amazon Lex is a completely managed service so you don?t have to manage scaling of resources or maintenance of code. Your interaction schema and language models are automatically backed up. We also provide comprehensive versioning capability for easy rollback. Amazon Lex architecture does not require storage or backups of end user data.
When do I use Amazon Polly vs. Amazon Lex?,Amazon Polly converts text inputs to speech. Amazon Lex is a service for building conversational interfaces using voice and text.
Does Amazon Lex get more intelligent over time?,Yes. Amazon Lex uses deep learning to improve over time.
"I was in the Amazon Lex preview program. &nbsp;Now that Amazon Lex is GA, what happens to my account?","On April 19, 2017, Amazon Web Services announced that Amazon Lex exited Preview and entered General Availability. As such, we will be terminating the Amazon Lex Preview Program on May 1, 2017. Usage will be charged as per the pricing plan starting May 1st. Your first 12 months for the free tier will start on May 1st. Please note that Amazon Lex is now supported under Developer Support, Business Support and Enterprise Support plans. You can also post your queries on the public Amazon Lex forums."
How do I create a bot in Amazon Lex?,"To create a bot, you will first define the actions performed by the bot. These actions are the intents that need to be fulfilled by the bot. For each intent, you will add sample utterances and slots. Utterances are phrases that invoke the intent. Slots are input data required to fulfill the intent. Lastly, you will provide the business logic necessary to execute the action. An Amazon Lex bot can be created both via Console and REST APIs."
Can I implement business logic on the client?,Yes. Amazon Lex provides the option of returning parsed intent and slots back to the client for business logic implementation.
How can I validate user input? ,"Amazon Lex provides deep integration with AWS Lambda and you can validate user input using the initialization and validation codeHook. This code gets executed at every turn of the conversation. The codehook can be used to set up session parameters, validate user input and customize responses."
What is an Intent? ,"To build an Amazon Lex bot, you will need to identify a set of actions - known as 'intents? -- that you want your bot to fulfill. A bot can have multiple intents. For example, a ?BookTickets? bot can have intents to make reservations, cancel reservations and review reservations."
What is an utterance? ,"An ?utterance? is the spoken or typed phrase to invoke an intent. For example, to invoke the intent to make reservations, you would provide a sample utterance such as, ?Can I make a reservation??"
What are slots? ,"To fulfill an intent, the Amazon Lex bot needs information from the user. This information is captured in ?slots?. For example, you would define show name and time as slots for intent to make reservations."
What are prompts? ,"Amazon Lex elicits the defined ?slots? by using the ?prompts? provided. For example, to elicit value for the slot ?time? you will define a prompt such as ?What show time would you like to reserve??. Amazon Lex is capable of eliciting multiple slot values via a multi-turn conversation."
How is an action fulfilled?,"Amazon Lex integrates with AWS Lambda for ?fulfillment? of the action or business logic. Alternately, you can configure Amazon Lex to return parsed intent and slot values to the client for action fulfillment."
How do I monitor and track my bot? ,"You can track metrics for your bot on the ?Monitoring? dashboard in the Amazon Lex Console. Currently, you can track the number of missed utterances, request latency and traffic by channel for your bot. You can view list of utterances that were not recognized by your bot, aka 'missed utterances'. With these monitoring capabilities, you view how your users are interacting with the bot and make improvements over time."
What happens when I ?build? a bot?,Building a bot triggers machine learning and creates the models for your bot. A new version of your intents and slot types is created. Once created a version is immutable.
How can I test an Amazon Lex bot?,You can test your Amazon Lex bot via the test window on the console. &nbsp;Any business logic implemented in AWS Lambda can be tested via this console as well. All supported browsers allow for testing text with your Amazon Lex bot; voice can be tested from a Chrome browser.
How can I create Amazon Lex bots for mobile?,"Amazon Lex provides SDKs for iOS and Android. You can develop bots for your mobile use cases with these SDKs. User authentication can be enabled via Amazon Cognito. You can use AWS Mobile Hub to build, test and monitor bots for your mobile platforms. AWS Mobile Hub can be used to automatically provision Amazon Lex bots from a template."
How can I make Amazon Lex bots available on messaging services?,"Amazon Lex bots can be published to messaging platforms like Facebook Messenger, Slack, Kik, and Twilio SMS. To publish the bot you can provide the tokens for authentication in the console, and we will store it securely and provide a callback URL that you can provide to the chat service."
Do I have to submit my bot for certification prior to deployment?,You don?t need to certify your bot with Amazon prior to deployment.
Can I have an Amazon Lex bot version deployed for use by end users while I continue to develop on a different version?,"Yes. You can build and deploy a version of your bot into production while you continue to develop on a different version. Every version of an Amazon Lex bot will have an ARN. Each version can be associated with a different alias. You can use these tools to set up dev, stage and prod environments"
Can I choose different versions while deploying to different messaging services?,"Yes. You can deploy a specific version to each messaging service. Every version of Amazon Lex will have an ARN. Each version can be associated with an alias. You can use different aliases for deployment to different messaging service. Also, you can have multiple bots deployed to the same messaging service."
What is the maximum duration of speech input?,Amazon Lex supports up to 15 seconds of speech input.
Can I configure for speech input and text output?,"Yes, you can just choose the PostContent API to provide voice input and choose text output."
How many languages are supported on Amazon Lex?,"Currently, Amazon Lex supports US English."
What audio formats does Amazon Lex support?,"Amazon Lex supports the following formats for input audio: LPCM and Opus; Supported output audio formats: MPEG, OGG, PCM."
Can I use Amazon Lex in VPC?,"Amazon Lex can be accessed from VPC via public endpoints for building and running a bot. Currently, Amazon Lex does not provide a VPC endpoint."
Can I access Amazon Lex bots locally i.e. without an Internet connection?,No. End users will need to access the Amazon Lex runtime endpoint over the Internet.
How is this different from Alexa Skills Kit?,"Alexa Skills Kit (ASK) is used to build skills for use in the Alexa ecosystem and devices and lets developers take advantage of all Alexa capabilities such as the Smart Home and Flash Briefing API, streaming audio and rich GUI experiences. Amazon Lex bots support both voice and text and can be deployed across mobile and messaging platforms."
Do I need a wake word to invoke an Amazon Lex intent?,"Amazon Lex does not support wake word functionality. The app that integrates with Amazon Lex will be responsible for triggering the microphone, i.e. push to talk."
Can an Amazon Lex bot respond using Alexa?s voice?,"Currently we do not support the Alexa voice for Amazon Lex responses. However, there are 7 other voices from which to choose."
Can I create an Alexa Skill from an Amazon Lex bot?,"Amazon Lex provides the ability for you to export your Amazon Lex bot schema into a JSON file that is compatible with Amazon Alexa. Once downloaded as JSON, you need to log in to the Alexa developer portal, navigate to the ?Interaction Model? tab, launch the Alexa Skill Builder, and paste the bot schema into the Code Editor of your Alexa Skill.&nbsp; More details and steps can be found in the Amazon Lex documentation."
"When exporting my Amazon Lex bot schema to use in an Alexa skill, are my AWS Lambda functions exported and included in the bot schema?",No. Only the bot definition will be downloaded.
I have created an Alexa Skill from an Amazon Lex bot using the schema export feature. Which Alexa platforms support the Amazon Lex bot schema?,"All Alexa platforms that support Alexa skills can be used: The Amazon Echo, Amazon Dot, Amazon Look, Amazon Tap, Amazon Echo Show and any third-party Alexa-enabled devices."
"Are voice and text inputs processed by Amazon Lex stored, and how are they used by AWS?","Amazon Lex may store and use voice and text inputs processed by the service solely to provide and maintain the service and to improve and develop the quality of Amazon Lex and other Amazon machine-learning/artificial-intelligence technologies. Use of your content is necessary for continuous improvement of your Amazon Lex customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Can I delete voice and text inputs stored by Amazon Lex?,Yes. You can request deletion of voice and text inputs associated with your account by contacting AWS Support. Deleting voice and text inputs may degrade your Amazon Lex experience.
Who has access to my content that is processed and stored by Amazon Lex?,"Only authorized employees will have access to your content that is processed by Amazon Lex. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see?https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Do I still own my content that is processed and stored by Amazon Lex?,You always retain ownership of your content and we will only use your content with your consent.
"Can I use Amazon Lex in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Lex Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Lex in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13. Amazon Lex does not store or retain voice or text utterance information from websites, programs, or applications that are identified by customers in accordance with the Amazon Lex Service Terms as being directed or targeted, in whole or in part, to children under age 13 and subject to COPPA."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13. whole or in part, to children under age 13."
What SDKs are supported for Amazon Lex?,"Amazon Lex currently supports SDKs for runtime services. IoS and Android SDKs, as well as Java, JS, Python, CLI, .Net, Ruby, PHP, Go, and CPP support both text and speech input."
Can I use SDKs to build bots?,"You can build bots using SDKs: Java, JavaScript, Python, CLI, .NET, Ruby on Rails, PHP, Go, and CPP."
Which enterprise connectors are supported on Amazon Lex?,"Amazon Lex integrates with enterprise connectors via AWS Lambda. The following enterprise connectors can be provisioned via AWS Mobile Hub: Salesforce, Microsoft Dynamics, Marketo, Zendesk, Quickbooks, and HubSpot."
What support is provided for Amazon Lex?,"Depending on your AWS support contract, Amazon Lex is supported under Developer Support, Business Support and Enterprise Support plans. &nbsp;You can also post your queries on the Amazon Lex forums."
How does Amazon Lex count the number of requests?,"Every input to an Amazon Lex bot is counted as a request. For example, if an end user provides 5 inputs to the bot as part of conversation, these are billed as 5 requests. Usage is metered and billed per request."
How much does Amazon Lex cost?,It is free to get started. Please see Amazon Lex Pricing Page for current pricing information.
Does Amazon Lex participate in the AWS Free Tier?,"Yes. You can try Amazon Lex for free. From the date you get started with Amazon Lex, you can process up to 10,000 text requests and 5,000 speech requests per month for free during the first year."
"I was in the Amazon Lex preview program. Now that Amazon Lex is GA, what happens to my account?","On April 19, 2017, Amazon Web Services announced that Amazon Lex exited Preview and entered General Availability. As such, we will be terminating the Amazon Lex Preview Program on May 1, 2017. Usage will be charged as per the pricing plan starting May 1st. Your first 12 months for the free tier will start on May 1st. Please note that Amazon Lex is now supported under Developer Support, Business Support and Enterprise Support plans. You can also post your queries on the public Amazon Lex forums."
What is Amazon Transcribe?,"Amazon Transcribe is an AWS service that makes it easy for customers to convert speech-to-text. Using Automatic Speech Recognition (ASR) technology, customers can choose to use?Amazon Transcribe for a variety of business applications, including transcription of voice-based customer service calls, generation of subtitles on audio/video content, and conduct (text based) content analysis on audio/video content. ?"
How does Amazon Transcribe interact with other AWS products?,"Amazon Transcribe converts audio input into text, which opens the door for various text analytics applications on voice input. For instance, by using Amazon Comprehend on the converted text data from Amazon Transcribe, customers can perform sentiment analysis or extract entities and key phrases. Similarly, by integrating with Amazon Translate and Amazon Polly, customers can accept voice input in one language, translate it into another and generate voice output, effectively enabling multi-lingual conversations. It is also possible to integrate Amazon Transcribe with Amazon Elasticsearch to index and perform text based search across audio/video library.?"
What else should I know before using Amazon Transcribe service?,"Amazon Transcribe service is designed to handle a wide range of speech and acoustic characteristics, including variations in volume, pitch, and speaking rate. The quality and content of the audio signal (including but not limited to factors such as background noise, overlapping speakers, accented speech, or switches between languages within a single audio file) may affect the accuracy of service output. We are constantly updating the service to improve its ability to accommodate additional acoustic variation and content types.?"
How will developers access Transcribe?,"The easiest way to get started with Amazon Transcribe is to submit a job using the console to transcribe an audio file. You can also call the service directly from the AWS Command Line Interface, or use one of the supported SDKs of your choice to integrate with your applications. Either way, you can start using Amazon Transcribe to generate automated transcripts for your audio files with just a few lines of code."
What kind of inputs does Amazon Transcribe support?,"Amazon Transcribe supports both 16 kHz and 8kHz audio streams, and multiple audio encodings, including WAV, MP3, MP4 and FLAC."
Does Amazon Transcribe support real-time transcriptions?,Yes. Amazon Transcribe enables users to open a bidirectional stream over HTTP2. Users can send an audio stream to the service while receiving a text stream in return in real time. ?
What encoding does real-time transcription support?,Streaming transcription currently supports 16-bit Linear PCM encoding.?
What languages does Amazon Transcribe support?,"For information on language support, please refer to this documentation page.?"
What devices does Amazon Transcribe work with?,"Amazon Transcribe for the most part is device agnostic. In general, Amazon Transcribe works with any device that includes an on-device microphone such as phones, PCs, tablets, and IoT devices (e.g. car audio systems). Amazon Transcribe API will be able to detect the quality of the audio stream being input at the device (8kHz VS 16kHz) and will appropriately select the acoustic models for converting speech-to-text. Furthermore, developers can call Transcribe API through their applications to access speech-to-text conversion capability.?"
Are there size restrictions on the audio content that Amazon Transcribe can process?,Amazon Transcribe service calls are limited to 4 hours (or 2GB) per API call for our batch service. The streaming service can accommodate open connections up to 4 hours long.?
What programming languages does Amazon Transcribe support?,"Amazon Transcribe batch service supports .NET, Go, Java, Javascript, PHP, Python and Ruby. Amazon Transcribe real-time service supports Java SDK, Ruby SDK, and C++ SDK. Additional SDK support are coming. For more details, visit the Resources page."
My custom vocabulary words are not being recognized! What can I do?,"The speech recognition output depends on a number of factors in addition to custom vocabulary entries, so there can be no assurance that if a term is included in the custom vocabulary, it will be correctly recognized. However, the most frequent reason is that a custom word lacks the correct pronunciation. If you haven?t provided a pronunciation for your custom word, please try to create one. If you already have provided one, double-check its correctness, or include other pronunciation variants if necessary. This can be done by creating multiple entries in the custom vocabulary file that differ in the pronunciation field."
Why do I see too many custom words in my output?,"Custom vocabularies are optimized for a small list of targeted words; larger vocabularies may lead to over-generation of custom words, especially when they contain words that are pronounced in a similar way. If you have a large list, please try reducing it to rare words and words that are actually expected to occur in your audio files. If you have a large vocabulary covering multiple use cases, split it into separate lists for different use cases. The words that are short and sound similar to many other words, may lead to over-generation (too many custom words appearing in the output). It is preferable to combine these words with surrounding words and list them as hyphen-separated phrases. For example, the custom word ?A.D.? could be included as part of a phrase such as ?A.D.-converter?."
"There are two ways of giving pronunciations, IPA or SoundsLike fields in the custom vocabulary table. Which one is better?","IPA allows for more precise pronunciations. You should provide IPA pronunciations if you are able to generate IPA (e.g., from a lexicon that has IPA pronunciations or an online converter tool)."
I'd like to use IPA but I'm not a linguistic expert. Is there an online tool I can use?,"Several standard dictionaries, such as the Oxford English Dictionary or the Cambridge Dictionary (including their online versions) provide pronunciations in IPA. There are also online converters (e.g. easypronunciation.com or tophonetics.com for English) ? however, note that in most cases these tools are based on underlying dictionaries and may not generate correct IPA for some words, such as proper names. Amazon Transcribe does not endorse any third-party tools."
Do I need to use different IPA standards that are specific to a different accents of the same language? (e.g. US English versus British English)?,"You should use the IPA standard that is appropriate for the audio files you will be processing ? e.g., if you are expecting to process audio from British English speakers, use the British English pronunciation standard. The set of allowed IPA symbols may differ for the different languages and dialects supported by Amazon Transcribe; please make sure that your pronunciations contain only the allowed characters. Details on the IPA character sets can be found in the documentation: https://docs.aws.amazon.com/transcribe/latest/dg/how-vocabulary.html#charsets"
How can I provide the pronunciation using SoundsLike field in the custom vocabulary table?,"You can break a word or phrase down into smaller pieces and provide a pronunciation for each piece using the standard orthography of the language to mimic the way that the word sounds. For example, in English you can provide pronunciation hints for the phrase Los-Angeles like this: loss-ann-gel-es. The hint for the word Etienne would look like this: eh-tee-en. You separate each part of the hint with a hyphen (-). You can use any of the allowed characters for the input language."
How do two different ways of providing acronyms (with periods and without periods but with pronunciations) work?,"If you use an acronym containing periods, the spelling pronunciation will be generated internally. If you do not use periods, please provide the pronunciation in the pronunciation field. For some acronyms, it is not obvious whether they have a spelling pronunciation or a word-like pronunciation (e.g., NATO is often pronounced ?n e? t o?? (nay-toh) rather than ??n e? ti o?? (N. A. T. O.))."
Where can I find examples of how to use custom pronunciations?,You can find sample input formats and examples in the documentation: https://docs.aws.amazon.com/transcribe/latest/dg/how-vocabulary.html.
"What happens if I use the wrong IPA? If I am uncertain, am I better off not inputting any IPA?","The system will use the pronunciation you provide; this should increase the likelihood of the word being recognized correctly if the pronunciation is correct and matches what was spoken. If you are not certain you are generating correct IPA, please run a comparison by processing your audio files with a vocabulary that contains your IPA pronunciations, and with a vocabulary that only contains the words (and, optionally, display-as forms). If you do not provide any pronunciations the service will use an approximation, which may or may not work better than your input."
"When using DisplayAs forms, can I display character sets unrelated to the original language being transcribed? (e.g. output ?Street? as ????).","Yes. While phrases may only use a restricted set of characters for the specific language, UTF-8 characters apart from \t (TAB) are permitted in the DisplayAs column."
Is Automatic content redaction available with both batch and streaming APIs for Transcribe?,"No, it is only available for batch APIs at this time."
What languages are supported for Automatic content redaction?,US-English (en-US) is supported at this time.
Does Automatic content redaction also redact sensitive personal information from the source audio?,"No, this feature does not remove sensitive personal information from the source audio. You can however redact personal information from the source audio yourself using the start and end timestamps that are provided in the redacted transcripts for each instance of an identified PII utterance."
Can I use Automatic content redaction for redacting personal information from the existing text transcripts?,"No, Automatic content redaction only works on audio file as an input."
What else should I know before using Automatic content redaction?,"Automatic content redaction is designed to identify and remove personally identifiable information (PII), but due to the predictive nature of machine learning, it may not identify and remove all instances of PII in a transcript generated by the service. You should review any output provided by Automatic content redaction to ensure it meets your needs."
What does it cost?,Refer to the Amazon Transcribe Pricing page to learn more.
What AWS regions are available for Amazon Transcribe?,Please refer to the AWS Global Infrastructure Region Table.
"Are voice inputs processed by Amazon Transcribe stored, and how are they used by AWS?","Amazon Transcribe may store and use voice inputs processed by the service solely to provide and maintain the service and to improve and develop the quality of Amazon Transcribe and other Amazon machine-learning/artificial-intelligence?technologies. Use of your content is important for continuous improvement of your Amazon Transcribe customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services, or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority, and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information.?You may opt out of having your content used to improve and develop the quality of Amazon Transcribe and other Amazon machine-learning/artificial-intelligence technologies by contacting AWS Support."
Can I delete data and artifacts associated with transcription jobs stored by Amazon Transcribe?,"Yes. You can use available Delete APIs to delete data and other artifacts associated with transcription jobs. If you have issues doing so, contact AWS support."
Who has access to my content that is processed and stored by Amazon Transcribe?,"Only authorized employees will have access to your content that is processed by Amazon Transcribe. Your trust, privacy, and the security of your content are our highest priority, and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Do I still own my content that is processed and stored by Amazon Transcribe?,"You always retain ownership of your content, and we will only use your content with your consent."
Is the content processed by Amazon Transcribe moved outside the AWS region where I am using Amazon Transcribe?,"Any content processed by Amazon Transcribe is encrypted and stored at rest in the AWS region where you are using Amazon Transcribe. Some portion of content processed by Amazon Transcribe may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Transcribe customer experience and other Amazon machine-learning/artificial-intelligence technologies. If you opt out of having your content used to develop the quality of Amazon Transcribe and other Amazon machine-learning/artificial-intelligence technologies by contacting AWS Support, your content will not be stored in another AWS region. You can request deletion of voice inputs associated with your account by contacting AWS Support. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
"Can I use Amazon Transcribe in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Transcribe Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Transcribe in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
What is Amazon Transcribe Medical?,"Amazon Transcribe Medical is an automatic speech recognition (ASR) service that makes it easy for developers to add medical speech-to-text capabilities to their applications. Using Amazon Transcribe Medical, you can quickly and accurately transcribe medical dictation and conversational speech into text for a variety of purposes, such as recording physician notes or processing in downstream text analytics to extract meaningful insights."
What can I do with Amazon Transcribe Medical?,"Amazon Transcribe Medical uses advanced machine learning models to accurately transcribe medical speech into text. Transcribe Medical can general text transcripts that can be used to support a variety of use cases, spanning clinical documentation workflow and drug safety monitoring (pharmacovigilance) to subtitling for telemedicine and even contact center analytics in the healthcare and life sciences domains."
Do I need to be an expert in automatic speech recognition (ASR) to use Amazon Transcribe Medical?,"No, you don?t need any ASR or machine learning expertise to use Amazon Transcribe Medical. You only need to call Transcribe Medical?s API, and the service will handle the required machine learning in the backend to transcribe medical speech to text."
How do I get started with Amazon Transcribe Medical?,You can get started with Amazon Transcribe Medical from the AWS Management console or by using the SDK. Please refer to this technical documentation page for details. Amazon Transcribe Medical provides a free tier so that you can test the service. Refer to this pricing page for more information.
Which languages does Amazon Transcribe Medical support?,Amazon Transcribe Medical currently supports medical transcription in US English.
Which medical specialties does Amazon Transcribe Medical support?,"Amazon Transcribe Medical supports transcription for primary care, covering specialties such as family medicine, internal medicine, pediatrics, and OB-GYN."
In which AWS regions is Amazon Transcribe Medical available?,"Amazon Transcribe Medical is currently available in US-East (N. Virginia), US East (Ohio), US West (Oregon), Canada (Central), EU (Ireland), and Asia Pacific (Sydney)."
How is Amazon Transcribe Medical priced?,Refer to the Amazon Transcribe Medical pricing page to learn more about pricing details.
Is Amazon Transcribe Medical HIPAA eligible?,Yes.
Is the content processed by Amazon Transcribe Medical used for any purpose other than to provide the service?,Amazon Transcribe Medical does not use content processed by the service for any reason other than to provide and maintain the service. Content processed by the service is not used to develop or improve the quality for Amazon Transcribe Medical or any other Amazon machine-learning/artificial-intelligence technologies.
Does Amazon Transcribe Medical learn over time?,"Yes, Amazon Transcribe Medical uses machine learning and is continuously being trained to make it better for customer use cases. Amazon Transcribe medical does not store or use customer data used with the service to train the models"
What else should I know before using the Amazon Transcribe Medical service?,"Amazon Transcribe Medical is not a substitute for professional medical advice, diagnosis, or treatment. You and your end users are responsible for exercising your and their own discretion, experience, and judgment in determining the correctness, completeness, timeliness, and suitability of any information provided by Amazon Transcribe Medical. You and your end users are solely responsible for any decisions, advice, actions, and/or inactions based on the use of Amazon Transcribe Medical. You are responsible for reviewing any output provided by Amazon Transcribe Medical to ensure it meets your needs."
What is Amazon Rekognition?,"Amazon Rekognition is a service that makes it easy to add powerful visual analysis to your applications. Rekognition Image lets you easily build powerful applications to search, verify, and organize millions of images. Rekognition Video lets you extract motion-based context from stored or live stream videos and helps you analyze them. Rekognition Image is an image recognition service that detects objects, scenes, and faces; extracts text; recognizes celebrities; and identifies inappropriate content in images. It also allows you to search and compare faces. Rekognition Image is based on the same proven, highly scalable, deep learning technology developed by Amazon?s computer vision scientists to analyze billions of images daily for Prime Photos.

Rekognition Image uses deep neural network models to detect and label thousands of objects and scenes in your images, and we are continually adding new labels and facial recognition features to the service. With Rekognition Image, you only pay for the images you analyze and the face metadata you store.

Rekognition Video is a video recognition service that detects activities; understands the movement of people in frame; and recognizes objects, celebrities, and inappropriate content in videos stored in Amazon S3 and live video streams from Acuity. Rekognition Video detects persons and tracks them through the video even when their faces are not visible, or as the whole person might go in and out of the scene. For example, this could be used in an application that sends a real-time notification when someone delivers a package to your door. Rekognition Video allows you also to index metadata like objects, activities, scene, celebrities, and faces that make video search easy."
What is deep learning?,"Deep learning is a sub-field of Machine Learning and a significant branch of Artificial Intelligence. It aims to infer high-level abstractions from raw data by using a deep graph with multiple processing layers composed of multiple linear and non-linear transformations. Deep learning is loosely based on models of information processing and communication in the brain. Deep learning replaces handcrafted features with ones learned from very large amounts of annotated data. Learning occurs by iteratively estimating hundreds of thousands of parameters in the deep graph with efficient algorithms. Several deep learning architectures such as convolutional deep neural networks (CNNs), and recurrent neural networks have been applied to computer vision, speech recognition, natural language processing, and audio recognition to produce state-of-the-art results on various tasks.

Amazon Rekognition is a part of the Amazon AI family of services. Amazon AI services use deep learning to understand images, turn text into lifelike speech, and build intuitive conversational text and speech interfaces."
Do I need any deep learning expertise to use Amazon Rekognition?,"No. With Amazon Rekognition, you don?t have to build, maintain or upgrade deep learning pipelines. To achieve accurate results on complex computer vision tasks such as object and scene detection, face analysis, and face recognition, deep learning systems need to be tuned properly and trained with massive amounts of labeled ground truth data. Sourcing, cleaning, and labeling data accurately is a time-consuming and expensive task. Moreover, training a deep neural network is computationally expensive and often requires custom hardware built using Graphics Processing Units (GPU).

Amazon Rekognition is fully managed and comes pre-trained for image and video recognition tasks, so that you don?t have invest your time and resources on creating a deep learning pipeline. Amazon Rekognition continues to improve the accuracy of its models by building upon the latest research and sourcing new training data. This allows you to focus on high-value application design and development."
What are the most common use cases for Amazon Rekognition?,"The most common use-cases for Rekognition Image include: 
    Searchable Image Library
    Face-Based User Verification
    Sentiment Analysis
    Facial Recognition
    Image Moderation

The most common use-cases for Rekognition Video include:

    Search Index for video archives
    Easy filtering of video for explicit and suggestive content
"
How do I get started with Amazon Rekognition?,"If you are not already signed up for Amazon Rekognition, you can click the ""Try Amazon Rekognition"" button on the Amazon Rekognition page and complete the sign-up process. You must have an Amazon Web Services account; if you do not already have one, you will be prompted to create one during the sign-up process. Once you are signed up, try out Amazon Rekognition with your own images and videos using the Amazon Rekognition Management Console or download the Amazon Rekognition SDKs to start creating your own applications. Please refer to our step-by-step Getting Started Guide for more information."
What APIs does Amazon Rekognition offer??,"Amazon Rekognition Image offers APIs to detect objects and scenes, detect and analyze faces, recognize celebrities, detect inappropriate content, and search for similar faces in a collection of faces, along with APIs to manage resources. Rekognition Image also offers APIs to compare faces and extract text, while Rekognition Video also offers APIs to track persons and manage live stream video from Acuity. For details, please refer to the Amazon Rekognition API Reference."
What image and video formats does Amazon Rekognition support??,"Amazon Rekognition Image currently supports the JPEG and PNG image formats. You can submit images either as an S3 object or as a byte array. Amazon Rekognition Video operations can analyze videos stored in Amazon S3 buckets. The video must be encoded using the H.264 codec. The supported file formats are MPEG-4 and MOV. A codec is software or hardware that compresses data for faster delivery and decompresses received data into its original form. The H.264 codec is commonly used for the recording, compression and distribution of video content. A video file format may contain one or more codecs. If your MOV or MPEG-4 format video file does not work with Rekognition Video, check that the codec used to encode the video is H.264."
What file sizes can I use with Amazon Rekognition??,"Amazon Rekognition Image supports image file sizes up to 15MB when passed as an S3 object, and up to 5MB when submitted as an image byte array. Amazon Rekognition Video supports up to 10 GB files and up to 6 hour videos when passed through as an S3 file."
How does image resolution affect the quality of Rekognition Image API results ??,"Amazon Rekognition works across a wide range of image resolutions. For best results we recommend using VGA (640x480) resolution or higher. Going below QVGA (320x240) may increase the chances of missing faces, objects, or inappropriate content; although Amazon Rekognition accepts images that are at least 80 pixels in both dimensions."
How small can an object be for Amazon Rekognition Image to detect and analyze it?,"As a rule of thumb, please ensure that the smallest object or face present in the image is at least 5% of the size (in pixels) of the shorter image dimension. For example, if you are working with a 1600x900 image, the smallest face or object should be at least 45 pixels in either dimension."
How can I get Amazon Rekognition predictions reviewed by humans?,"Amazon Rekognition is directly integrated with Amazon Augmented AI (Amazon A2I) so you can easily route low confidence predictions from Amazon Rekognition Image to human reviewers. Using the Amazon Rekognition API for content moderation or the Amazon A2I console, you can specify the conditions under which Amazon A2I routes predictions to reviewers, which can be either a confidence threshold or a random sampling percentage. If you specify a confidence threshold, Amazon A2I routes only those predictions that fall below the threshold for human review. You can adjust these thresholds at any time to achieve the right balance between accuracy and cost-effectiveness. Alternatively, if you specify a sampling percentage, Amazon A2I routes a random sample of the predictions for human review. This can help you implement audits to monitor the prediction accuracy regularly. Amazon A2I also provides reviewers with a web interface consisting of all the instructions and tools they need to complete their review tasks. For more information about implementing human review with Amazon Rekognition, see the Amazon A2I webpage."
How does video resolution affect the quality of Rekognition Video API? results??,"The system is trained to recognize faces larger than 32 pixels (on the shortest dimension), which translate into a minimum size for a face to be recognized that varies from approximately 1/7 of the screen smaller dimension at QVGA resolution to 1/30 at HD 1080p resolution. For example, at VGA resolution, users should expect lower performances for faces smaller than 1/10 of the screen smaller dimension."
What else can affect the quality of the Rekognition Video APIs ??,"Besides video resolution, heavy blur, fast moving persons, lighting conditions, pose may affect the quality of the APIs."
What is the preferred user video content that is suitable for Rekognition Video APIs??,"This API works best with consumer and professional videos taken with frontal field of view in normal color and lighting conditions. This API is not tested for black and white, IR or extreme lighting condition. Applications that are sensitive to false alarms are advised to discard outputs with confidence score below a selected (application-specific) confidence score."
In which AWS regions is Amazon Rekognition available??,"For a list of all regions where Amazon Rekognition is available, see the AWS Region table."
What is a label?,"A label is an object, scene, or concept found in an image based on its contents. For example, a photo of people on a tropical beach may contain labels such as ?Person?, ?Water?, ?Sand?, ?Palm Tree?, and ?Swimwear? (objects), ?Beach? (scene), and ?Outdoors? (concept).?"
What is a confidence score and how do I use it?,"A confidence score is a number between 0 and 100 that indicates the probability that a given prediction is correct. In the tropical beach example, if the object and scene detection process returns a confidence score of 99 for the label ?Water? and 35 for the label ?Palm Tree?, then it is more likely that the image contains water but not a palm tree. Applications that are very sensitive to detection errors (false positives) should discard results associated with confidence scores below a certain threshold. The optimum threshold depends on the application. In many cases, you will get the best user experience by setting minimum confidence values higher than the default value."
What is Object and Scene Detection?,"Object and Scene Detection refers to the process of analyzing an image or video to assign labels based on its visual content. Amazon Rekognition Image does this through the DetectLabels API. This API lets you automatically identify thousands of objects, scenes, and concepts and returns a confidence score for each label. DetectLabels uses a default confidence threshold of 50. Object and Scene detection is ideal for customers who want to search and organize large image libraries, including consumer and lifestyle applications that depend on user-generated content and ad tech companies looking to improve their targeting algorithms."
Can Amazon Rekognition detect object locations and return bounding boxes?,"Yes, Amazon Rekognition can detect the location of many common objects such as ?Person?, ?Car?, ?Gun?, or ?Dog? in both images and videos. You get the coordinates of the bounding rectangle for each instance of the object found, as well as a confidence score. For more details on the API response structure for object bounding boxes, please refer to the documentation."
Does Amazon Rekognition provide information on the relationship between detected labels?,"Yes, for every label found, Amazon Rekognition returns the parent labels if they exist. Parents are returned in hierarchical order - the leftmost label is the immediate parent or synonym, while following labels are parents of parents. For example, when a ?Car? is identified, Amazon Rekognition returns two parent labels ?Vehicle? (parent), and ?Transportation? (parent?s parent). For more details on how to use hierarchical taxonomy relations, please refer to the documentation."
What types of labels does Amazon Rekognition support? ?,"Rekognition supports thousands of labels belonging to common categories including, but not limited to:People and Events: ?Wedding?, ?Bride?, ?Baby?, ?Birthday Cake?, ?Guitarist?, etc.
Food and Drink: ?Apple?, ?Sandwich?, ?Wine?, ?Cake?, ?Pizza?, etc.
Nature and Outdoors: ?Beach?, ?Mountains?, ?Lake?, ?Sunset?, ?Rainbow?, etc.
Animals and Pets: ?Dog?, ?Cat?, ?Horse?, ?Tiger?, ?Turtle?, etc.
Home and Garden: ?Bed?, ?Table?, ?Backyard?, ?Chandelier?, ?Bedroom?, etc.
Sports and Leisure: ?Golf?, ?Basketball?, ?Hockey?, ?Tennis?, ?Hiking?, etc.
Plants and Flowers: ?Rose?, ?Tulip?, ?Palm Tree?, ?Forest?, ?Bamboo?, etc.
Art and Entertainment: ?Sculpture?, ?Painting?, ?Guitar?, ?Ballet?, ?Mosaic?, etc.
Transportation and Vehicles: ?Airplane?, ?Car?, ?Bicycle?, ?Motorcycle?, ?Truck?, etc.
Electronics: ?Computer?, ?Mobile Phone?, ?Video Camera?, ?TV?, ?Headphones?, etc."
How is Object and Scene Detection different for video analysis?,"Rekognition Video enables you to automatically identify thousands of objects - such as vehicles or pets - and activities - such as celebrating or dancing - and provides you with timestamps and a confidence score for each label. It also relies on motion and time context in the video to accurately identify complex activities, such as ?blowing a candle? or ?extinguishing fire?."
I can?t find the label I need. How do I request a new label?,Please send us your requests through AWS Customer Support. Amazon Rekognition continuously expands its catalog of labels based on customer feedback
How can you check if Amazon Rekognition has updated its models?,Amazon Rekognition returns a LabelModelVersion parameter that lets you know whether the model has been updated. Object and Scene detection models are updated frequently based on customer feedback.
"Can I use Custom Labels for analyzing faces, customized text detection, or finding unsafe image content?","No. Custom Labels is meant for finding objects and scenes in images. Custom Labels is not designed for analyzing faces, customized text detection, or finding unsafe image content. You should use other Rekognition APIs for these tasks. Please refer to the documentation for face analysis, Text detection, and Moderation."
How many images are needed to train a custom model?,"The number of images required to train a custom model depends on the variability of the custom labels you want the model to predict and the quality of the training data. For example, a distinct logo overlaid on an image can be detected with 1-2 training images, while a more subtle logo required to be detected under many variations (scale, viewpoint, deformations) may need in the order of tens to hundreds of training examples with high quality annotations. If you already have a high number of labeled images, we recommend training a model with as many images as you have available. Please refer to the documentation for limits on maximum training dataset size. Although hundreds of images may sometimes be required to train a custom model with high accuracy, with Custom Labels you can first train a model with tens of images per label, review your test results to understand where it does not work, and accordingly add new training images and train again to iteratively improve your model."
How many inference compute resources should I provision for my custom model?,"The number of parallel inference compute resources needed depends on how many images you need to process at a given point in time. The throughput of a single resource will depend factors including the size of the images, the complexity of those images (how many detected objects are visible), and the complexity of your custom model. We recommend that you monitor the frequency at which you need provision your custom model, and the number of images that need to be processed at a single time, in order to schedule provisioning of your custom model most efficiently. If you expect to process images periodically (e.g. once a day or week, or scheduled times during the day), you should Start provisioning your custom model at a scheduled time, process all your images, and then Stop provisioning. If you don?t stop provisioning, you will be charged even if no images are processed."
My training has failed. Will I be charged?,No. You will not be charged for the compute resources if your training fails.
What is Unsafe Content Detection?,"Amazon Rekognition?s Unsafe Content Detection is a deep-learning based easy to use API for detection of explicit or suggestive adult content, violent content, weapons, and visually disturbing content in image and videos. Beyond flagging an image or video based on presence of unsafe content, Amazon Rekognition also returns a hierarchical list of labels with confidence scores. These labels indicate specific sub-categories of the type of unsafe content detected (such as violence), thus providing more granular control to developers to filter and manage large volumes of user generated content (UGC). This API can be used in moderation workflows for applications such as social and dating sites, photo sharing platforms, blogs and forums, apps for children, e-commerce site, entertainment and online advertising services."
What types of unsafe content does Amazon Rekognition detect?,"Amazon Rekognition detects the following types of explicit and suggestive adult content in images and videos: Explicit Nudity:

    Nudity
    Graphic Male Nudity
    Graphic Female Nudity
    Sexual Activity
    Illustrated Nudity Or Sexual Activity
    Adult Toys

Suggestive:

    Female Swimwear Or Underwear
    Male Swimwear Or Underwear
    Partial Nudity
    Revealing Clothes

Amazon Rekognition returns a hierarchy of labels, as well as a confidence score for each detected label. For instance, given an inappropriate image, Rekognition may return ?Explicit Nudity? with a confidence score as a top level label. Developers could just use this to flag content. In the same response, Rekognition also returns second level of granularity by providing additional context like ?Graphic Male Nudity? with its own confidence score. Developers could use this information to build more complex filtering logic to serve different geographies and demographics.

In addition, Amazon Rekognition can detect the following types of violent and visually disturbing content:

Violence:

    Graphic Violence Or Gore
    Physical Violence
    Weapons Violence
    Weapons
    Self Injury

Visually Disturbing:

    Emaciated Bodies
    Corpses
    Hanging

Please note that the Unsafe Image Detection API is not an authority on, or in any way purports to be an exhaustive filter of explicit or suggestive adult content, violent content, weapons, and visually disturbing content. Furthermore, this API does not detect whether an image includes illegal content (such as child pornography) or unnatural adult content."
"Can Amazon Rekognition?s Unsafe Content Detection API detect other inappropriate content besides adult, violent and visually disturbing content?","Currently, Rekognition only supports the labels we have outlined above. We are working to continuously add and improve labels based on feedback from our customers. If you require other types of inappropriate content to be detected in images, please reach out to us using the feedback process outlined later in this section."
How can I know which model version I am currently using?,"Amazon Rekognition makes regular improvement to its models. To keep track of model version, you can use the 'ModerationModelVersion' field in the API response."
How can I ensure that Amazon Rekognition meets accuracy goals for my unsafe image or video detection use case?,"Amazon Rekognition?s Unsafe Content Detection models have been and tuned and tested extensively, but we recommend that you measure the accuracy on your own data sets to gauge performance. You can use the ?MinConfidence? parameter in your API requests to balance detection of content (recall) vs the accuracy of detection (precision). If you reduce ?MinConfidence?, you are likely to detect most of the inappropriate content, but are also likely to pick up content that is not actually explicit or suggestive. If you increase ?MinConfidence? you are likely to ensure that all your detected content is actually explicit or suggestive but some inappropriate content may not be tagged. For examples on how to use ?MinConfidence? for images, please refer to the documentation here."
How can I give feedback to Rekognition to improve its Unsafe Content Detection??,Please send us your requests through AWS Customer Support. Amazon Rekognition continuously expands the types of inappropriate content detected based on customer feedback. Please note that illegal content (such as child pornography) will not be accepted through this process.
What is Facial Analysis?,"Facial analysis is the process of detecting a face within an image and extracting relevant face attributes from it. Amazon Rekognition Image takes returns the bounding box for each face detected in an image along with attributes such as gender, presence of sunglasses, and face landmark points. Rekognition Video will return the faces detected in a video with timestamps and, for each detected face, the position and a bounding box along with face landmark points."
What face attributes can I get from Amazon Rekognition?,"Amazon Rekognition returns the following facial attributes for each face detected, along with a bounding box and confidence score for each attribute:Gender
Smile
Emotions
Eyeglasses
Sunglasses
Eyes open
Mouth open
Mustache
Beard
Pose
Quality
Face landmarks"
What is face pose?,"Face pose refers to the rotation of a detected face on the pitch, roll, and yaw axes. Each of these parameters is returned as an angle between -180 and +180 degrees. Face pose can be used to find the orientation of the face bounding polygon (as opposed to a rectangular bounding box), to measure deformation, to track faces accurately, and more."
What is face quality?,"Face quality describes the quality of the detected face image using two parameters: sharpness and brightness. Both parameters are returned as values between 0 and 1. You can apply a threshold to these parameters to filter well-lit and sharp faces. This is useful for applications that benefit from high-quality face images, such as face comparison and face recognition."
What are face landmarks???,"Face landmarks are a set of salient points, usually located on the corners, tips or mid points of key facial components such as the eyes, nose, and mouth. Amazon Rekognition DetectFaces API returns a set of face landmarks that can be used to crop faces, morph one face into another, overlay custom masks to create custom filters, and more."
How many faces can I detect in an image?,You can detect up to 100 faces in an image using Amazon Rekognition.
How is Facial Analysis different for video analysis?,"With Rekognition Video, you can locate faces across a video and analyze face attributes, such as whether the face is smiling, eyes are open, or showing emotions. Rekognition Video will return the detected faces with timestamps and, for each detected face, the position and a bounding box along with landmark points such as left eye, right eye, nose, left corner of the mouth, and right corner of the mouth. This position and time information can be used to easily track user sentiment over time and deliver additional functionality such as automatic face frames, highlights, or crops."
"In addition to Video resolution, what else can affect the quality of the Rekognition Video APIs?","Besides video resolution, the quality and representative faces, part of the face collections to search, has major impact. Using multiple face instances per person with variations like beard, glasses, poses (profile and frontal) will significantly improve the performance. Typically very fast moving people and blurred videos may experience lower quality."
What is Face Comparison??,"Face Comparison is the process of comparing one face to one or more faces to measure similarity. Using the CompareFaces API, Amazon Rekognition Image lets you measure the likelihood that faces in two images are of the same person. The API compares a face in the source input image with each face detected in the target input image and returns a similarity score for each comparison. You also get a bounding box and confidence score for each face detected. You can use face comparison to verify a person?s identity against their personnel photo on file in near real-time."
Can I use a source image with more than one face??,"Yes. If the source image contains multiple faces, CompareFaces detects the largest face and uses it to compare with each face detected in the target image."
How many faces can I compare against?,You can compare one face in the source image with up to 15 detected faces in the target image.
What is Facial Recognition?,"Facial recognition is the process of identifying or verifying a person?s identity by searching for their face in a collection of faces. Using facial recognition, you can easily build applications such as multi-factor authentication for bank payments, automated building entry for employees, and more."
What is a face collection and how do I create one? ?,"A face collection is a searchable index of face feature vectors, owned and managed by you. Using the CreateCollection API, you can easily create a collection in a supported AWS region and get back an Amazon Resource Name (ARN). Each face collection has a unique CollectionId associated with it."
How do I add faces to or delete faces from a face collection? ?,"To add a face to an existing face collection, use the IndexFaces API. This API accepts an image in the form of an S3 object or image byte array and adds a vector representation of the faces detected to the face collection. IndexFaces also returns a unique FaceId and face bounding box for each of the faces added. To delete a face from an existing face collection, use the DeleteFaces API. This API operates on the face collection supplied (using a CollectionId) and removes the entries corresponding to the list of FaceIds. For more information on adding and deleting faces, please refer to our Managing Collections example."
How do I search for a face within a face collection?,"Once you have created an indexed collection of faces, you can search for a face within it using either an image (SearchFaceByImage) or a FaceId (SearchFaces). These APIs take in an input face and return a set of faces that match, ordered by similarity score with the highest similarity first. For more details, please refer to our Searching Faces example."
How is Facial Recognition different for video analysis?,"Rekognition Video allows you to perform real time face searches against collections with tens of millions of faces. First, you create a face collection, where you can store faces, which are vector representations of facial features. Rekognition then searches the face collection for visually similar faces throughout your video. Rekognition will return a confidence score for each of the faces in your video, so you can display likely matches in your application."
In addition to Video resolution what else can affect the quality of the Video APIs ?,"Besides video resolution, the quality and representative faces part of the face collections to search has major impact. Using multiple face instances per person with variations like beard, glasses, poses (profile and frontal) will significantly improve the performance. Typically very fast moving people may experience low recall. In addition, blurred videos may also experience lower quality."
What is Celebrity Recognition?,"Amazon Rekognition?s Celebrity Recognition is a deep learning based easy-to-use API for detection and recognition of individuals who are famous, noteworthy, or prominent in their field. The RecognizeCelebrities API has been built to operate at scale and recognize celebrities across a number of categories, such as politics, sports, business, entertainment, and media. Our Celebrity Recognition feature is ideal for customers who need to index and search their digital image libraries for celebrities based on their particular interest."
Who can be identified by the Celebrity Recognition API?,"Amazon Rekognition can only identify celebrities that the deep learning models have been trained to recognize. Please note that the RecognizeCelebrities API is not an authority on, and in no way purports to be, an exhaustive list of celebrities. The feature has been designed to include as many celebrities as possible, based on the needs and feedback of our customers. We are constantly adding new names, but the fact that Celebrity Recognition does not recognize individuals that may be deemed prominent by any other groups or by our customers is not a reflection of our opinion of their celebrity status. If you would like to see additional celebrities identified by Celebrity Recognition, please submit feedback."
Can a celebrity identified through the Amazon Rekognition API request to be removed from the feature?,"Yes. If a celebrity wishes to be removed from the feature, he or she can send an email to AWS Customer Support and we will process the removal request."
What sources are supported to provide additional information about a Celebrity ?,"The API supports an optional list of sources to provide additional information about the celebrity as a part of the API response. We currently provide the IMDB URL, when it is available. We may add other sources at a later date."
How is Celebrity Recognition different for video analysis??,"With Rekognition Video, you can detect and recognize when and where well known persons appear in a video. The time-coded output includes the name and unique id of the celebrity, bounding box coordinates, confidence score, and URLs pointing to related content for the celebrity, for example, the celebrity's IMDB link. The celebrity is also detected even if sometimes the face becomes occluded in the video. This feature allows you to index and search digital video libraries for use cases related to your specific marketing and media needs."
"In addition to Video resolution, what else can affect the quality of the Rekognition Video APIs??","Very fast moving celebrities and blurred videos can affect the quality of the Rekognition Video APIs. In addition, heavy makeup and camouflage common for actors/actresses, can also affect the quality."
What is Text Detection??,"Text detection is a capability of Amazon Rekognition that allows you to detect and recognize text within an image or a video, such as street names, captions, product names, overlaid graphics, video subtitles, and vehicular license plates. Text detection is specifically built to work with real-world images and videos, rather than document images. Amazon Rekognition?s DetectText API takes in an image and returns the text label and a bounding box for each detected string of characters, along with a confidence score. For example, in image sharing and social media applications, you can enable visual search based on an index of images that contain the same text labels. In security applications, you can identify vehicles based on license plate numbers from images taken by traffic cams. Similarly, for videos, using the StartTextDetection and GetTextDetection APIs, you can detect text and get confidence scores and timestamps for each detection. In media and entertainment applications, you can create text metadata to support search for relevant content, such as news, sport scores, commercials, and captions. You can also review the detected text for policy or compliance violations e.g. an email address or phone number that has been overlaid by spammers."
What type of text does Amazon Rekognition Text Detection support??,"Text Detection is specifically built to work with real-world images and videos rather than document images. It supports text in most Latin scripts and numbers embedded in a large variety of layouts, fonts and styles, and overlaid on background objects at various orientation as banners and posters. Text detection recognizes up to 50 sequences of characters per the image or video frame and lists them as words and lines. Text detection supports text rotated by up to -90 to +90 degrees from the horizontal axis."
Can I limit text detection to specific regions in an image or video frame? ,"Yes, you can use text detection filtering options to specify up to 10 Regions of Interest (ROIs) in the API request. Amazon Rekognition will only return text that falls within these regions.?"
Can I filter text detections by word confidence or bounding box size? ,"Yes, in the API request you can use the text detection filtering options to specify thresholds for minimum confidence scores or minimum bounding box dimensions."
How can I give feedback to Rekognition to improve its text recognition?,Please send us your requests through AWS Customer Support. Amazon Rekognition continuously expands the types of text content recognized based on customer feedback.
What types of entities can Amazon Rekognition Video detect? ,"Amazon Rekognition Video can detect objects, scenes, faces, celebrities, text, and inappropriate content in videos. You can also search for faces appearing in a video using your own repository or collection of face images."
What types file formats and codecs does Amazon Rekognition Video support? ,"Amazon Rekognition Video supports H.264 files in MPEG-4 (.mp4) or MOV format. If your video files use a different codec, you can transcode them into H.264 using AWS Elemental MediaConvert."
How do Amazon Rekognition Video asynchronous APIs work? ,"Amazon Rekognition Video can process videos stored in an Amazon S3 bucket. You can use an asynchronous set of operations: you start video analysis by calling a Start operation such as StartLabelDetection for detecting objects and scenes. The completion status of the request is published to an Amazon Simple Notification Service (SNS) topic. To get the completion status from the Amazon SNS topic, you can use an Amazon Simple Queue Service (SQS) queue or an AWS Lambda function. Once you have the completion status, you call a Get operation such as GetLabelDetection to get the results of the request. For a list of available Amazon Rekognition Video APIs, please see this page."
How can I find the timeline for each detection in a video? ,"Amazon Rekognition Video returns timestamps in milliseconds for each entity it detects on the video timeline, for example, using the label detection, a ?Dog? may be found at 166ms from the start of a video. Using this information, you can understand the timeline of an object or a celebrity shown in the video. To see how a sample API response with timestamps looks like, please refer to this page on detecting object and scenes in Amazon Rekognition Video."
How many concurrent video analysis jobs can I run with Amazon Rekognition Video? ,"You can process up to 20 concurrent jobs with Amazon Rekognition Video. For more details on limits, please see our limits page."
What video resolution should I use?,"Amazon Rekognition Video automatically handles a wide range of video resolutions and video quality. We recommend using a 720p (1280?720 pixels) to 1080p (1920x1080 pixels), or their equivalent resolutions in other aspect ratios for optimum results. Very low resolution (such as QVGA or 240p) and very low-quality videos may adversely impact results quality."
How can I analyze videos in real time? ,"In streaming mode, you can search faces against a collection of face images in real time. Rekognition Video face search APIs natively integrate with video stream from Amazon Kinesis Video Streams, a service that enables developers to transmit thousands of live feeds and associated metadata. For enterprise security applications, this makes real-time detection of Persons of Interest easy and accurate."
Does Amazon Rekognition Video work with Amazon Kinesis Video Streams? ,"Rekognition Video uses a Kinesis Video Stream as input, to process a video stream. The analysis results are output from Rekognition Video to a Kinesis data stream and finally read by your client application. Rekognition Video provides a stream processor you can use to start and manage the analysis of streaming video. To learn more, please refer to Working with Streaming Videos."
What is People Pathing? ,"With Rekognition Video, you can find the path of each person across the video timeline. Rekognition Video detects persons even when the camera is in motion and, for each person, returns a bounding box and the face, along with face attributes and timestamps. For retail applications, this allows to generate customer insights anonymously, such as how customers move across aisles in a shopping mall or how long they are waiting in checkout lines."
What types of media analysis can Amazon Rekognition Video perform? ,"Amazon Rekognition Video can currently perform four types of common operational media analysis tasks: 
    Black frames detection: Videos often contain a short duration of empty black frames with no audio that are used as cues to insert advertisements, or to demarcate the end of a program segment such as a scene or the opening credits. With Amazon Rekognition Video, you can detect such black frame sequences to automate ad insertion, package content for VOD, and demarcate various program segments or scenes. Black frames with audio (such as fade outs or voiceovers) are considered as content and not returned.
    End credits detection: Amazon Rekognition Video helps you automatically identify the exact frames where the closing credits start and end for a movie or TV show. With this information, you can generate markers for interactive viewer prompts such as ?Next Episode? in VOD applications, or find out the last frame of program content in a video. Amazon Rekognition Video is trained to handle a wide variety of end credit styles ranging from simple rolling credits to more challenging credits alongside content, and excludes opening credits automatically.
    Shot detection: A shot is a series of interrelated consecutive pictures taken contiguously by a single camera and representing a continuous action in time and space. With Amazon Rekognition Video, you can detect the start, end, and duration of each shot, as well as account for all the shots in a piece of content. Shot metadata can be used for applications such as creating promotional videos using selected shots, generating a set of preview thumbnails that avoid transitional content between shots, and inserting ads in spots that don?t disrupt viewer experience, such as the middle of a shot when someone is speaking.
    Color Bars detection: Amazon Rekognition Video allows you to detect sections of video that display SMPTE color bars, which are a set of colors displayed in specific patterns to ensure color is calibrated correctly on broadcast monitors, programs, and on cameras. This metadata is useful to prepare content for VOD applications by removing color bar segments from the content, or to detect issues such as loss of broadcast signals in a recording, when color bars are shown continuously as a default signal instead of content.

Amazon Rekognition Video provides the start, end, duration, and timecodes for each detected entity.

"
How do I get started with media analysis using Amazon Rekognition Video?,"Media analysis features are available through the Amazon Rekognition Video segment detection API. This is an asynchronous API composed of two operations: StartSegmentDetection to start the analysis, and GetSegmentDetection to get the analysis results. To get started, please refer to the documentation. If you want to visualize the results of media analysis or even try out other Amazon AI services like Amazon Transcribe with your own videos, please use the Media Insights Engine (MIE) ? a serverless framework to easily generate insights and develop applications for your video, audio, text, and image resources, using AWS Machine Learning and Media services. You can easily spin up your own MIE instance using the supplied AWS CloudFormation template, and then use the sample application linked in the ?Outputs? tab of your AWS CloudFormation console to try out your own videos and visualize analysis results."
What is a frame accurate timecode?,"Frame accurate timecodes provide the exact frame number for a relevant segment of video or entity. Media companies commonly process timecodes using the SMPTE (Society of Motion Picture and Television Engineers) format hours:minutes:seconds:frame number, for example, 00:24:53:22."
Is Amazon Rekognition Video segment detection frame accurate?,"Yes, the Amazon Rekognition Video segment detection API provides frame accurate SMPTE timecodes, as well as millisecond timestamps for the start and end of each detection."
What types of frame rate formats can Amazon Rekognition Video segment detection handle? ,"Amazon Rekognition Video segment detection automatically handles integer, fractional and drop frame standards for frame rates between 15 and 60fps. For example, common frame rates such as 23.976 fps, 25fps, 29.97 fps and 30fps are supported by segment detection. Frame rate information is utilized to provide frame accurate timecodes in each case."
How does Amazon Rekognition count the number of images processed?,"For APIs that accept images as inputs, Amazon Rekognition counts the actual number of images analyzed as the number of images processed. DetectLabels, DetectModerationLabels, DetectFaces, IndexFaces, RecognizeCelebrities, and SearchFaceByImage belong to this category. For the CompareFaces API, where two images are passed as input, only the source image is counted as a unit of images processed. For API calls that don?t require an image as an input parameter, Amazon Rekognition counts each API call as one image processed. SearchFaces belongs to this category.

The remaining Amazon Rekognition APIs - ListFaces, DeleteFaces, CreateCollection, DeleteCollection, and ListCollections - do not count towards images processed."
How does Amazon Rekognition count the number of minutes of videos processed?,"For archived videos, Amazon Rekognition counts the minutes of video that is successfully processed by the API and meters them for billing. For Live stream videos you get charged in chunks of every five seconds of video that we successfully process."
Which APIs does Amazon Rekognition charge for?,"Amazon Rekognition Image charges for the following APIs: DetectLabels, DetectModerationLabels, DetectText, DetectFaces, IndexFaces, RecognizeCelebrities, SearchFaceByImage, CompareFaces, and SearchFaces. Amazon Rekognition Video charges are based on duration of video in minutes, successfully processed by StartLabelDetection, StartFaceDetection, StartFaceDetection, StartTextDetection, StartContentModeration, StartPersonTracking, StartCelebrityRecognition, StartFaceSearch and StartStreamProcessor APIs."
How much does Amazon Rekognition cost?,Please see the Amazon Rekognition Pricing Page for current pricing information.
Will I be charged for the feature vectors I store in my face collections?,"Yes. Amazon Rekognition charges $0.01 per 1,000 face vectors per month. For details, please see the pricing page."
Does Amazon Rekognition participate in the AWS Free Tier?,"Yes. As part of the AWS Free Usage Tier, you can get started with Amazon Rekognition for free. Upon sign-up, new Amazon Rekognition customers can analyze up to 5,000 images for free each month for the first 12 months. You can use all Amazon Rekognition APIs with this free tier, and also store up to 1,000 faces without any charge. In addition, Amazon Rekognition Video customers can analyze 1,000 minutes of Video free, per month, for the first year."
Do your prices include taxes?,"For details on taxes, please see Amazon Web Services Tax Help."
Does Amazon Rekognition Video work with images stored on Amazon S3?,"Yes. You can start analyzing images stored in Amazon S3 by simply pointing the Amazon Rekognition API to your S3 bucket. You don?t need to move your data. For more details of how to use S3 objects with Amazon Rekognition API calls, please see our Detect Labels exercise."
Can I use Amazon Rekognition with images stored in an Amazon S3 bucket in another region?,No. Please ensure that the Amazon S3 bucket you want to use is in the same region as your Amazon Rekognition API endpoint.
How do I process multiple image files in a batch using Amazon Rekognition?,You can process your Amazon S3 images in bulk using the steps described in our Amazon Rekognition Batch Processing example on GitHub.
How can I use AWS Lambda with Amazon Rekognition?,"Amazon Rekognition provides seamless access to AWS Lambda and allows you bring trigger-based image analysis to your AWS data stores such as Amazon S3 and Amazon DynamoDB. To use Amazon Rekognition with AWS Lambda, please follow the steps outlined here and select the Amazon Rekognition blueprint."
Does Amazon Rekognition work with AWS CloudTrail?,"Yes. Amazon Rekognition supports logging the following actions as events in CloudTrail log files: CreateCollection, DeleteCollection, CreateStreamProcessor, DeleteStreamProcessor, DescribeStreamProcessor, ListStreamProcessors, and ListCollections. For more details on the Amazon Rekognition API calls that are integrated with AWS CloudTrail, see Logging Amazon Rekonition API Calls with AWS CloudTrail."
"Are image and video inputs processed by Amazon Rekognition stored, and how are they used by AWS?","Amazon Rekognition may store and use image and video inputs processed by the service solely to provide and maintain the service and, unless you opt out as provided below, to improve and develop the quality of Amazon Rekognition and other Amazon machine-learning/artificial-intelligence technologies. Use of your content is important for continuous improvement of your Amazon Rekognition customer experience, including the development and training of related technologies. We do not use any personally identifiable information that may be contained in your content to target products, services or marketing to you or your end users. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information. You may opt out of having your image and video inputs used to improve or develop the quality of Amazon Rekognition and other?Amazon machine-learning/artificial-intelligence technologies by contacting AWS Support."
Can I delete image and video inputs stored by Amazon Rekognition?,Yes. You can request deletion of image and video inputs associated with your account by contacting AWS Support. Deleting image and video inputs may degrade your Amazon Rekognition experience.
Who has access to my content that is processed and stored by Amazon Rekognition?,"Only authorized employees will have access to your content that is processed by Amazon Rekognition. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information."
Do I still own my content that is processed and stored by Amazon Rekognition?,You always retain ownership of your content and we will only use your content with your consent.
Is the content processed by Amazon Rekognition moved outside the AWS region where I am using Amazon Rekognition?,"Any content processed by Amazon Rekognition is encrypted and stored at rest in the AWS region where you are using Amazon Rekognition. Unless you opt out as provided below, some portion of content processed by Amazon Rekognition may be stored in another AWS region solely in connection with the continuous improvement and development of your Amazon Rekognition customer experience and other Amazon machine-learning/artificial-intelligence technologies. You can request deletion of image and video inputs associated with your account by contacting AWS Support. Your trust, privacy, and the security of your content are our highest priority and we implement appropriate and sophisticated technical and physical controls, including encryption at rest and in transit, designed to prevent unauthorized access to, or disclosure of, your content and ensure that our use complies with our commitments to you. Please see https://aws.amazon.com/compliance/data-privacy-faq/ for more information. Your content will not be stored in another AWS region if you contact AWS Support to opt out of having your content used to improve and develop the quality of Amazon Rekognition and other Amazon machine-learning/artificial-intelligence technologies."
"Can I use Amazon Rekognition in connection with websites, programs or other applications that are directed or targeted to children under age 13 and subject to the Children?s Online Privacy Protection Act (COPPA)?","Yes, subject to your compliance with the Amazon Rekognition Service Terms, including your obligation to provide any required notices and obtain any required verifiable parental consent under COPPA, you may use Amazon Rekognition in connection with websites, programs, or other applications that are directed or targeted, in whole or in part, to children under age 13."
"How do I determine whether my website, program, or application is subject to COPPA?","For information about the requirements of COPPA and guidance for determining whether your website, program, or other application is subject to COPPA, please refer directly to the resources provided and maintained by the United States Federal Trade Commission. This site also contains information regarding how to determine whether a service is directed or targeted, in whole or in part, to children under age 13."
Is Amazon Rekognition a HIPAA Eligible Service?,"Amazon Rekognition is a HIPAA Eligible Service covered under the AWS Business Associate Addendum (AWS BAA). If you have an AWS BAA in place, Amazon Rekognition will use, disclose, and maintain your Protected Health Information (PHI) only as permitted by the terms of your AWS BAA."
How do I control user access for Amazon Rekognition?,"Amazon Rekognition is integrated with AWS Identity and Access Management (IAM). AWS IAM policies can be used to ensure that only authorized users have access to Amazon Rekognition APIs. For more details, please see the Amazon Rekognition Authentication and Access Control page."
How can I report potential Amazon Rekognition abuse?,"If you suspect that Amazon Rekognition is being used in manner that is abusive or illegal, or infringes on your rights or the rights of other people, please report this use and AWS will investigate the issue."
